2226	jim_gray	introduction	download	The_Benchmark_Handbook	
2241	jim_gray	database and transaction processing performance handbook		The_Benchmark_Handbook	
2328	jim_gray	the benchmark handbook for database and transaction systems lrb 1st edition rrb		null	
2329	jim_gray	the benchmark handbook for database and transaction systems lrb 2nd edition rrb	cloud computing promise a number of advantage for the deployment of dataintensive application one important promise be reduce cost with a payasyougo business model another promise be lrb virtually rrb unlimited throughput by add server if the workload increase this paper list alternative architecture to effect cloud computing for database application and report on the result of a comprehensive evaluation of exist commercial cloud service that have adopt these architecture the focus of this work be on transaction processing lrb ie read and update workload rrb rather than analytic or olap workload which have recently gain a great deal of attention the result be surprising in several way most importantly it seem that all major vendor have adopt a different architecture for they cloud service as a result the cost and performance of the service vary significantly depend on the workload doi 101145 18071671807231 cloud service rds vendor transaction processing ing	null	
2330	jim_gray andreas_reuter	transaction processing concept and technique		null	
17461	jim_gray	note on data base operating systems		Advanced_Course:_Operating_Systems	
25987	jim_gray	the revolution in database system architecture	database system architecture be undergo revolutionary change most importantly algorithm and datum be be unify by integrate programming language with the database system this give a extensible objectrelational system where nonprocedural relational operator manipulate object set couple with this each dbms be now a web service this have huge implication for how we structure application dbmss be now object container queue be the first object to be add these queue be the basis for transaction processing and workflow application future workflow system be likely to be build on this core datum cube and online analytic processing be now baked into most dbmss beyond that dbmss have a framework for datum mining and machine learning algorithm decision tree bayes net clustering and time series analysis be build in new algorithm can be add there be a rebirth of column store for sparse table and to optimize bandwidth text temporal and spatial datum access method along with they probabilistic reasoning have be add to database system allow approximate and probabilistic answer be essential for many application many believe that xml and xquery will be the main datum structure and access pattern database system must accommodate that perspective external datum increasingly arrive as stream to be compare to historical datum so streamprocessing operator be be add to the dbms publishsubscribe system invert the dataquery ratio incoming datum be compare against million of query rather than query search million of record meanwhile disk and memory capacity be grow much faster than they bandwidth and latency so the database system increasingly use huge main memory and sequential disk access these change mandate a much more dynamic query optimization strategy one that adapt to current condition and selectivity rather than have a static plan intelligence be move to the periphery of the network each disk and each sensor will be a competent database machine relational algebra be a convenient way to program these system database system be now expect to be selfmanaging selfhealing and alwaysup we researcher and developer have we work cut out for we in deliver all these feature dbm disk dbmss database systems	ADBIS_(Local_Proceedings)	
28520	donald_d._chamberlin jim_gray irving_l._traiger	view authorization and locking in a relational data base system	in the interest of brevity we assume that the reader be familiar with the notion of a relational datum base in particular we assume a familiarity with the work of codd or boyce and chamberlin the example in this paper will be draw from a data base which describe a department store and consist of three relation emp lrb name sal mgr dept rrb sales lrb dept item vol rrb loc lrb dept floor rrb doi 101145 14999491500032 sal data base	AFIPS_National_Computer_Conference	
63002	jim_gray pete_homan henry_f._korth ron_obermarck	a straw man analysis of the probability of waiting and deadlock in a database system		Berkeley_Workshop	
69411	jim_gray	super servers commodity computer cluster pose a software challenge	technology be push the fastest processor onto single massproduced chip standards be define a new level of integration the pizza box a one board computer with memory disk baseware and middleware these development fundamentally change the way we will build computer future design must leverage commodity product cluster of computer be the natural way to build future mainframe a simple analysis suggest that such machine will have thousand of processor give a teraop processing rate terabyte of ram storage many terabyte of disc storage and terabitspersecond of communication bandwidth this presage 4t cluster to a iron monger or software house the t stand for terror to customer it stand for tremendous these computer will be ideally suit to be superserver in future network software that extract parallelism from application be the key to make cluster useful clientserver computing have natural parallelism many client submit many independent request that can be process in parallel database visualization and scientific computing application have also make great stride in extract and exploit parallelism within a single application these promising first step bode well for cluster architecture the challenge remain to extend these technique to general purpose system ram disc commodity terabyte thousand of processor	BTW	
69875	jim_gray	why do computer stop and what can be about it		Buroautomation	
92243	allison_druin david_cavallo christopher_fabian benjamin_b._bederson glenda_revelle yvonne_rogers jim_gray	mobile technology for the world s child	in this panel academic nonprofit and industry professional will discuss they global perspective on	CHI_Extended_Abstracts	
94668	maria_a._nieto-santisteban jim_gray alexander_s._szalay james_annis aniruddha_r._thakar william_o'mullane	when database systems meet the grid	we illustrate the benefit of combine database system and grid technology for dataintensive application use a cluster of sql server we reimplement a exist grid application that find galaxy cluster in a large astronomical database the sql implementation run a order of magnitude faster than the earlier tclcfilebased implementation we discuss why and how grid application can take advantage of database system metadata large amount of datum database systems avalanche sql implementation workflow dataintensive application	CIDR	
109990	jim_gray pete_homan harald_sammer bob_good dieter_gawlick	one thousand transaction per second	transaction per second contrast the several company intend to provide generalpurpose processing system capable of one thousand transaction this paper survey the need for such system and approach be take by three different group tps transaction processing bank tpf	COMPCON	
109991	jim_gray chris_nyberg	desktop batch processing	today online transaction processing application can downsize from mainframe to microprocessor commodity database system operate system and hardware come of age in 1993they surpass the online transaction processing performance of proprietary solution there be linger doubt about downsize batch transaction processing application the doubt center on the ability of microprocessor hardware to handle the high io bandwidth require by batch processing and on doubt that microprocessor system offer the software service and utility key to batch processing application this paper review the impressive progress of make by commodity software and hardware in processing oltp workload the discussion be quantitative because the transaction processing performance council define a set of benchmark that characterize oltp and that quantify price and performance discussion then turn to batch transaction processing there be less consensus on the characteristic of batch transaction processing consequently much of the discussion focus on requirement the discussion end with some performance measurement of utility run on dec alpha axp microprocessor and on commodity disk these result indicate that microprocessor today have the capacity to process batch workload at mainframe speed we predict that over the next few year batchprocessing software exploit parallel processing will emerge this combine with commodity hardware will provide both superior performance and price performance downsizing and rightsize be drive by economics in particular the economy of scale there be 100000000 microprocessor in use while there be at most 50000 mainframe in use this create a diseconomy of scale the fix engineering cost associate with mainframe must be amortize across a few thousand unit these cost in excess of a billion dollar drive unit cost into the million the benefit of mainframe do not justify these huge fixed cost c gordon bell observe that there be seven computer class rank by price lsb 1 rsb the small population lrb righthand column rrb have large fixed cost spread over a few unit these fix cost make big machine disproportionately expensive to make these argument concrete consider the follow price and volume the high mainframe price reflect multibillion dollar engineering cost amortize across a few thousand unit similar argument apply to software bill joy observe that one should not write software for a platform with less than 100000 licens because the economics be terrible the engineering cost be spread across only a few unit and so be prohibitive when joy formulate this rule commodity mean 100000 unit today commodity mean one million or doi 101109 cmpcon 1994282922	COMPCON	Syst._Center Digital_Equipment_Corp. San_Francisco CA
160561	jim_gray raymond_a._lorie gianfranco_r._putzolu irving_l._traiger	granularity of lock and degree of consistency in a shared data base	the problem of choose the appropriate hranularit lrb size rrb of lockable object be introduce and the tradeoff between concurrency and overhead be discusses a lock protocol which allow simultaneous lock at various granularity by different transaction be present it be base on the introduction of additional lock mode besides the conventional share mode an5 exclusive mode a proof be give of the equivalence of this protocol to a conventional one next the issue of consistency in a shared environment be analyze this discussion be motivate by the realization that some exist datum base system use automatic lock protocol which insure protection only from certain type of inconsistency lrb for instance those arise from transaction backup rrb thereby automatically provide a limited degree of consistency four s consistency be introduce they can be roughly characterize as follow degree 0 protect other from you update degree i additionally provide protection from lose update degree 2 additionally provide protection from read incorrect datum itea and degree 3 additionally provide protection from read incorrect relationship among datum item lrb ie total protection rrb a discussion follow on the relationship of the four degree to lock protocol concurrency overhead recovery and transaction structure lastly these idea be compare with e x i be t i n g d a t a management system a important issue which arise in the design of a data dase management system be the choice of lockable unite ie the datum aggregate which be atomically lock to insure consistency example of lockable unit be area file individual record field value and interval of field value the choice of lockable unit present a tradeoff between concurrency and overhead which be related to the size or kranularit z of the unit themselves onthe one hand concurrency be increase if a fine lockable unit lrb for example a record or field rrb be choose such unit be appropriate for a simple transaction which access few record on the other hand a fine unit of lock would be costly for a complex transaction which access a large number of record such a transaction would have to set and r e s e t a large 365 concurrency serializability locking high availability tency	IFIP_Working_Conference_on_Modelling_in_Data_Base_Management_Systems	
160685	michael_stonebraker lawrence_a._rowe bruce_g._lindsay jim_gray michael_j._carey michael_l._brodie philip_a._bernstein david_beech	thirdgeneration database system manifesto the committee for advanced dbms function	we call the older hierarchical and network system first generation datftha system and refer to the current collection of relmional system as the second generation in this paper we consider the characteristic that must be rus fie by the next generation of datum managex which we call third generation database system we requirement be collect into three ba iq tenet along with 13 more detailed proposilion doi 101145 101077390001 database systems sql objectrelational model	DS-4	
200993	jim_gray	triumph sin and challenge of database benchmarking		ExpDB	
202466	jim_gray	greeting from a filesystem user		FAST	
210813	jim_gray michael_a._harrison	single pass precedence analysis lrb extended abstract rrb		FOCS	
220608	werner_vogels dan_dumitriu kenneth_p._birman rod_gamache mike_massa rob_short john_vert joe_barrera jim_gray	the design and architecture of the microsoft cluster service a practical approach to highavailability and scalability	microsoft cluster service lrb msc rrb extend the windows nt operating system to support highavailability service the goal be to offer a execution environment where offtheshelf server application can continue to operate even in the presence of node failure later version of msc will provide scalability via a node and application management system which allow application to scale to hundred of node in this paper we provide a detailed description of the msc architecture and the design decision that have drive the implementation of the service the paper also describe how some major application use the msc feature and describe feature add to make it easier to implement and manage faulttolerant application on msc doi 101109 ftcs 1998689494 msc ibm dcom operating nent	FTCS	Dept._of_Comput._Sci. Cornell_Univ. Ithaca NY
220647	jim_gray	a comparison of the byzantine agreement problem and the transaction commit problem	transaction commit and byzantine agreement solve the problem of multiple process reach agreement in the presence of process and message failure this paper summarize the computation and fault model of the two kind of agreement and show the difference between they in particular it explain that byzantine agreement be rarely use in practice because it involve significantly more hardware and message yet do not give predictable behavior if there be more than a few error	Fault-Tolerant_Distributed_Computing	
255807	jim_gray	fault tolerance in tandem systems		HPTS	
255808	jim_gray	why tplite will dominate the tp market		HPTS	
268123	jim_gray	a transaction model		ICALP	
299651	surajit_chaudhuri ashok_k._chandra umeshwar_dayal jim_gray michael_stonebraker gio_wiederhold moshe_y._vardi	database research lead follow or get out of the way panel abstract		ICDE	
300027	jim_gray adam_bosworth andrew_layman hamid_pirahesh	datum cube a relational aggregation operator generalize groupby crosstab and subtotal		ICDE	
300028	jim_gray prashant_j._shenoy	rule of thumb in data engineering	this paper reexamine the rule of thumb for the design of datum storage system briefly it look at storage processing and network cost ratio and trend with a particular focus on performance and priceperformance amdahl s ratio law for system design need only slight revision after 35 year the major change be the increase use of ram a analysis also indicate storage should be use to cache both database and web datum to save disk bandwidth network bandwidth and people s time surprisingly the 5minute rule for disk caching become a cacheeverything rule for web caching doi 101109 icde 2000839382 mbp thumb inch	ICDE	
398104	jim_gemmell eve_m._schooler jim_gray	fcast multicast file distribution tune in download and drop out	reliable datum multicast be difficult to scale fcast file multicast combine multicast with forward error correction lrb fec rrb to solve this problem like classic multicast fcast scale to large audience and like other fec scheme it use bandwidth very efficiently some of the benefit of this combination be know previously but fcast contribute new caching method that improve disk throughput and new optimization for small file transfer congestion sender retransmission night disk	IMSA	
472721	jim_gray	online science the worldwide telescope as a prototype for the new computational science	computational science have historically mean simulation but there be a increase role for analysis and mining of online scientific datum as a case in point half of the world s astronomy datum be public the astronomy community be put all that datum on the internet so that the internet become the world s best telescope it have the whole sky in many band and in detail as good as the best 2yearold telescope it be useable by all astronomer everywhere this be the vision of the virtual observatoryalso call the world wide telescope lrb wwt rrb as one step along that path i have be work with the sloan digital sky survey lrb especially alex szalay of johns hopkins rrb and caltech to federate they datum in web service on the internet and to make it easy to ask question of the database lrb see httpskyserversdssorg rrb this talk explain the rationale for the wwt discuss how we design the database and talk about some datum mining task it also describe computer science challenge of publishing federate and mining scientific datum and argue that xml web service be key to federate diverse datum source doi 101145 956750956752 mining talk wwt	KDD	Microsoft_Research
544016	jim_gray	the cost of message	background there be strong evidence show that voluntary medical male circumcision lrb vmmc rrb reduce hiv incidence in man to inform the vmmc policy and goal of 13 priority country in eastern and southern africa we estimate the impact and cost of scale up adult vmmc use update countryspecific datum method and finding we use the decision maker program planning tool lrb dmppt rrb to model the impact and cost of scale up adult vmmc in botswana lesotho malawi mozambique namibia rwanda south africa swaziland tanzania uganda zambia zimbabwe and nyanza province in kenya we use epidemiologic and demographic datum from recent household survey for each country the cost of vmmc range from us 6585 to us 9515 per vmmc perform base on a cost assessment of vmmc service align with the world health organization s consideration of model for optimize volume and efficiency result from the dmppt model suggest that scale up adult vmmc to reach 80 coverage in the 13 country by 2015 would entail perform 2034 million circumcision between 2011 and 2015 and a additional 842 million between 2016 and 2025 lrb to maintain the 80 coverage rrb such a scaleup would result in avert 336 million new hiv infection through 2025 in addition while the model show that this scaleup would cost a total of us 2 billion between 2011 and 2025 it would result in net savings lrb due to avert treatment and care cost rrb amount to us 1651 billion conclusion this study suggest that rapid scaleup of vmmc in eastern and southern africa be warrant base on the likely impact on the region s hiv epidemic and net savings scale up of safe vmmc in eastern and southern africa will lead to a substantial reduction in hiv infection in the country and lower health system cost through avert hiv care cost doi 101371 journalpmed 1001132 hiv infection men and woman unaids scaleup vmmc	PODC	
575797	naga_k._govindaraju scott_larsen jim_gray dinesh_manocha	memory a memory model for scientific algorithm on graphic processor	we present a memory model to analyze and improve the performance of scientific algorithm on graphic process unit lrb gpus rrb we memory model be base on texturing hardware which use a 2d blockbased array representation to perform the underlie computation we incorporate many characteristic of gpu architecture include smaller cache size 2d block representation and use the 3c s model to analyze the cache miss moreover we present technique to improve the performance of nested loop on gpus in order to demonstrate the effectiveness of we model we highlight its performance on three memoryintensive scientific application sorting fast fourier transform and dense matrixmultiplication in practice we cacheefficient algorithm for these application be able to achieve memory throughput of 30 50 gbs on a nvidia 7900 gtx gpu we also compare we result with prior gpubased and cpubased implementation on highend processor in practice we be able to achieve 2 5 performance improvement doi 101109 sc 20062 gpu memory model gpubased gpus scientific algorithms	SC	
584544	stuart_ozer jim_gray alexander_s._szalay andreas_terzis razvan_musaloiu-elefteri katalin_szlavecz randal_c._burns joshua_cogan	data analysis tool for sensorbased science	science be increasingly drive by datum collect automatically from array of inexpensive sensor the collect datum volume require a different approach from the scientist current excel spreadsheet storage and analysis model spreadsheet work well for small datum set but scientist want high level summary of they datum for various statistical analysis without sacrifice the ability to drill down to every bit of the raw datum this demonstration describe we prototype data analysis system that be suitable for browse and visualization like a spreadsheet but scalable to much larger datum set doi 101145 11828071182844 spreadsheet	SenSys	Microsoft_Research
597076	susanne_englert jim_gray terrye_kocher praful_shah	a benchmark of nonstop sql release 2 demonstrate nearlinear speedup and scaleup on large database	nonstop sql be a implementation of ansiiso sql on tandem computer system in its second release nonstop sql transparently and automatically implement parallelism within a sql statement this parallelism allow query execution speed to increase almost linearly as processor and disc be add to the systemspeedup in addition this parallelism can help job restrict to a fix batch window when the job double in size its elapsed processing time will not change if proportionately more equipment be available to process the jobscaleup this paper describe the parallelism feature of nonstop sql and a audited benchmark that demonstrate these speedup and scaleup claim doi 101145 9845798766 speedup disc batch oltp sql	SIGMETRICS	
597143	jim_gray	a view of database system performance measure	database system allow quick creation of performance problem the goal of database system be to allow the computerilliterate to write complex and complete application it be the job of the system to translate a highlevel description of datum and procedure into efficient algorithm the real performance metric of a system be how successfully it meet these goal practitioner use a much narrower definition of system performance they assume a standard workload and measure performance by peak throughput and by dollar cost per transaction although many vendor have 8220 private 8221 performance measure bitton dewitt and turbyfill be the first to publish a measure of database system performance lsb bitton rsb they measure here call the wisconsin benchmark consist of a database design a set of 32 retrieval and update statement and a script for multiuser test they give two performance metric the elapsed time for each statement and the throughput of the system when run sixteen simultaneous script no response time requirement or cost measure be include in the definition the wisconsin benchmark be the most widely use database benchmark largely in response to the wisconsin benchmark a informal group include bitton and dewitt define a benchmark more representative of transaction processing application lsb anon rsb its workload be list item scan a minibatch operation to sequentially copy 1000 record item item sort a batch operation to sort one million record item item debitcredit a short transaction with terminal input and output via x 25 presentation service and a mix of five database access item list the debitcredit transaction have rule for scale the terminal network and database size as the transaction rate increase and also rule for distribute transaction if the system be decentralize the performance metric for this benchmark be list item elapsed time for the scan and sort item item peak throughput for the debitcredit transaction at 1 second response time for 95 of the transaction this give a tps lrb transaction per second rrb rating item item price per transaction where price be the 5year cost of hardware software and maintenance this be sometimes call the vendorsview of price item list this benchmark have be adopt by several vendor to compare they performance and price performance from release to release and also to compare they performance to competitive product mips whetstone and megaflop have serve a similar role in the scientific community a system s tps rating indicate not just processor speed but also io architecture operate system datum communication and database software performance unfortunately it do not capture easeofuse work continue on formalize these benchmark at present they be write in english ultimately they should be define by a file generator and a set of program write in a standard database language such as cobolsql when a vendor first measure he system against these benchmark the result be usually terrible both benchmark be design to expose generic performance bug in frequently use transaction processing atom for example the wisconsin and scan benchmark heavily penalize a system which be slow to read the next record in a file a system with poor performance on these benchmark can be analyze as follow most vendor have a 8220 atomic 8221 model of they system which represent each transaction as a collection of atom the atom be the primitive of the system for example the scan benchmark be represent by most vendor as scan begin transaction perform 1000 times read sequential insert sequential commit transaction the atomic weight for begin read sequential insert sequential and commit be measure for each release the atomic weight usually consist of cpu instruction message byte and disc io for a 8220 typical 8221 call to that operation these weight can be convert to service time by know the speed and utilization of the device lrb processor disc line rrb use for the application the molecular weight and service time of scan can then be compute as the sum of the atomic weight define and measure a system s atom be valuable it produce a simple conceptual model of how the system be use atomic measurement also expose performance bug for example base on the scan benchmark most system perform read sequential in 1000 instruction and with 02 disc io if a system use many more instruction or many more io then it have a performance problem similarly the debitcredit transaction typically consume about 2ooki lrb thousand instruction rrb and five disc io per transaction one system be know to use 800ki and 14 io per transaction the vendor could use atomic measurement to find the cause of such poor performance when such problem be localized to a atom solution to the problem readily suggest themselves so atomic measurement be useful for performance assurance and performance improvement atomic measurement also have a major role in system size and in capacity planning if the customer can describe he application in term of atom then a spreadsheet application can give he a estimate of the cpu disc and line cost for the application with substantially more effort lrb and assumption rrb the system s response time can be predict with even more effort a prototype system can be generate and benchmark from the atomic transaction description snapshot lsb stewart rsb and envision lsb envison rsb be example of system which combine atomic modeling queue modeling and ultimately benchmarking of real system generate from the atomic description of the application doi 101145 2990329905 formance ment vendor script scan	SIGMETRICS	
598040	hal_berenson philip_a._bernstein jim_gray jim_melton elizabeth_j._o'neil patrick_e._o'neil	a critique of ansi sql isolation level	ansi sql92 lsb ms ansi rsb define isolation i level i in term of i phenomenon i dirty read nonrepeatable read and phantoms this paper show that these phenomenon and the ansi sql definition fail to properly characterize several popular isolation level include the standard lock implementation of the level cover ambiguity in the statement of the phenomenon be investigate and a more formal statement be arrive at in addition new phenomenon that better characterize isolation type be introduce finally a important multiversion isolation type call snapshot isolation be define doi 101145 223784223785 ansi serializability isolation level snapshot isolation ple	SIGMOD_Conference	
598098	tom_barclay donald_r._slutz jim_gray	terraserver a spatial data warehouse	microsoft 174 terraserver store aerial satellite and topographic image of the earth in a sql database available via the internet it be the world s largest online atla combine eight terabyte of image datum from the united states geological survey lrb usgs rrb and spin2 internet browser provide intuitive spatial and text interface to the datum user need no special hardware software or knowledge to locate and browse imagery this paper describe how terabyte of 8220 internet unfriendly 8221 geospatial image be scrub and edit into hundred of million of 8220 internet friendly 8221 image tile and load into a sql datum warehouse all metadata and imagery be store in the sql database terraserver demonstrate that generalpurpose relational database technology can manage large scale image repository and show that web browser can be a good geospatial image presentation system doi 101145 342009335424	SIGMOD_Conference	
598654	naga_k._govindaraju jim_gray ritesh_kumar dinesh_manocha	gputerasort high performance graphic coprocessor sort for large database management	we present a novel external sorting algorithm use graphic processor lrb gpus rrb on large database compose of billion of record and wide key we algorithm use the datum parallelism within a gpu along with task parallelism by schedule some of the memoryintensive and computeintensive thread on the gpu we new sorting architecture provide multiple memory interface on the same pc a fast and dedicated memory interface on the gpu along with the main memory interface for cpu computation as a result we achieve higher memory bandwidth as compare to cpubased algorithm run on commodity pc we approach take into account the limited communication bandwidth between the cpu and the gpu and reduce the datum communication between the two processor we algorithm also improve the performance of disk transfer and achieve close to peak io performance we have test the performance of we algorithm on the sortbenchmark and apply it to large database compose of a few hundred gigabyte of datum we result on a 3 ghz pentium iv pc with 300 nvidia 7800 gt gpu indicate a significant performance improvement over optimize cpubased algorithm on highend pc with 36 ghz dual xeon processor we implementation be able to outperform the current highend pennysort benchmark and result in a higher performance to price ratio overall we result indicate that use a gpu as a coprocessor can significantly improve the performance of sort algorithm on large database doi 101145 11424731142511 sort gpgpu bitonic cpu gpus	SIGMOD_Conference	Microsoft_Research
598685	jim_gray	the next database revolution	database system architecture be undergo revolutionary change most importantly algorithm and datum be be unify by integrate programming language with the database system this give a extensible objectrelational system where nonprocedural relational operator manipulate object set couple with this each dbms be now a web service this have huge implication for how we structure application dbmss be now object container queue be the first object to be add these queue be the basis for transaction processing and workflow application future workflow system be likely to be build on this core datum cube and online analytic processing be now baked into most dbmss beyond that dbmss have a framework for datum mining and machine learning algorithm decision tree bayes net clustering and time series analysis be build in new algorithm can be add there be a rebirth of column store for sparse table and to optimize bandwidth text temporal and spatial datum access method along with they probabilistic reasoning have be add to database system allow approximate and probabilistic answer be essential for many application many believe that xml and xquery will be the main datum structure and access pattern database system must accommodate that perspective external datum increasingly arrive as stream to be compare to historical datum so streamprocessing operator be be add to the dbms publishsubscribe system invert the dataquery ratio incoming datum be compare against million of query rather than query search million of record meanwhile disk and memory capacity be grow much faster than they bandwidth and latency so the database system increasingly use huge main memory and sequential disk access these change mandate a much more dynamic query optimization strategy one that adapt to current condition and selectivity rather than have a static plan intelligence be move to the periphery of the network each disk and each sensor will be a competent database machine relational algebra be a convenient way to program these system database system be now expect to be selfmanaging selfhealing and alwaysup we researcher and developer have we work cut out for we in deliver all these feature doi 101145 10075681007570 dbmss database systems column store dbm disk	SIGMOD_Conference	
598687	jim_gray	practical problem in data management a position paper	database theory seem to lag behind or be orthogonal to the practical problem of database d88ign and implementation in my own specialty of transaction managwdsnt algorithma appear in work system five year before thsy the literatura the best ica papers explain the algorithm cxposa subtle bug in theaor generaliar thsm mo8t researchqr8 do not view svstem description as part of the literaiure this be true both in research lrb they do not read system in publication lrb if descripti rrb and a be well docuiiqnt8d in some system but be not publish in a journal the idea be viowsd as unpublished rrb a important practical pfoblem tobe solvad be to g8t theoretician8 to read the papers lrb manual rrb write by practitionqr i beleive this would inspire abstraction clarification and gencsalizatien of mechanism not fully think realizsd practitioner doi 101145 582192582196 ica	SIGMOD_Conference	
598694	jim_gray	operate system support for data management systems		SIGMOD_Conference	
598696	jim_gray	database performance metric		SIGMOD_Conference	
598698	jim_gray	database and transaction processing benchmark	this session be a compendium of the emerge performance and priceperformmce metic for database system and transaction processing system each kenchmark try to answer the question what computer should i buy clearly the answer question be the system that do the job with the lowest costofownership costofownership include project risk programming cost operation cost hardware cost and software cost it be difficult to quantify project risk programming cost and operation cost in contrast computer performance can be quantify and compare generic benchmark be often use in this way as a rough estimate of the relative system performance because the cost of implement and measure a specific application on many different system be usually prohibitive certain genetic benchmark have become so widely recognize that vendor announce the performance of new product in term of those benchmark for example dec hp and ibm state the relative performance and priceperformance of new machine and system release by state they rating on the transaction processing performance council s benchmark tpc bm a this practice be become increasingly common no single metric can measure the performance of computer system on all application system performance vary enormously from one application domain to another each system be typically design for a few problem domain and may be incapable of perform other task for example most supercomputer lack database and transaction processing software and so be inappropriate for most business application domainspecific benchmark be a response to this diversity of computer system use each such benchmark specify a synthetic workload characterize typical application in that problem domain the performance and of this workload on various computer system then give a rough estimate of they relative performance on that problem domain this handbook contain seven domainspecific benchmark cover database and transaction processing system to be useful a domainspecific benchmark must meet four important criterion it must be relevant it must measure the peak performance and priceperformance of system when perform typical operation within that problem domain portable it should be easy to implement the benchmark on many different system and architecture scaleabkx the benchmark should apply to small and large computer system it should be possible to scale the benchmark up to larger system and to parallel computer system as computer performance and architecture evolve simple the benchmark must be understandable otherwise it will lack credibility this session present three such benchmark 1 the transaction processing performance doi 101145 130283130288 different systems problem domain transaction processing computer systems operations cost	SIGMOD_Conference	
598715	jim_gray	parallel database systems 101	this talk be tutorial and remedial cover the basic idea of parallel database system other talk in this track go into detail on one or another product so this talk just define common terminology and idea it begin by explain why hardware trend lrb many inexpensive disk dram and microprocessor rrb force we to build parallel computer and program they in parallel it describe the spectrum between sharednothing shareddisk and share memory system and report on recent progress in building scaleable shared memory system and highspeed interconnect parallelism trade money for time parallel system be not cheaper they be faster parallel system must obey two law lrb 1 rrb the parallel system must be faster than a serial system and lrb 2 rrb the parallel system must give nearlinear speedup and scaleup pipeline and partition parallelism be contrast and the nemesis of block operator startup interference and skew be define they limit speedup and scaleup oltp and decision support lrb datum warehouse rrb parallelism be contrast oltp technique and progress be briefly mention lrb tpca b c rrb but most of the talk focus on batch lrb dss rrb parallel query system lrb tpcd rrb the recently approve tpcd decision support database benchmark be review publish performance number will be give if they be available by then database organization lrb partitioning and indexing rrb will be review the presentation then turn to a whirlwind tour of parallelize the relational operator scan selectproject sort and join the many approach to join and indexing dss datum be review lrb with special attention to join index and bitmap technique rrb the talk contain one small original contribution a benchmark buyer guide it propose simple test that can permission to copy without fee all or part of this material be grant provide that the copy be not make or distribute for direct commercial advantage the acm copyright notice and the title of the publication and its date appear and notice be give that copyin be by permission of the association of computing y machinery o copy otherwise or to republish require a fee andor specific permission 436 be apply to a parallel database system in a hour as a simple sanity check these test be simpler than the tpcd benchmrwk that test the query optimizer and the ability to run multipleconcurrent task the first test just see if the system can scan datum at disk speed it start with a millionrecord wisconsin table lrb record be 208 byte of 16 field rrb doi 101145 223784223864 parallel systems indexing speedup talk oltp	SIGMOD_Conference	
598717	jim_gray pat_helland patrick_e._o'neil dennis_shasha	the dangers of replication and a solution	update anywhereanytimeanyway transactional replication have unstable behavior as the workload scale up a tenfold increase in node and traffic give a thousand fold increase in deadlock or reconciliation master copy replication lrb primary copy rrb scheme reduce this problem a simple analytic model demonstrate these result a new twotier replication algorithm be propose that allow mobile lrb disconnect rrb application to propose tentative update transaction that be later apply to a master copy commutative update transaction avoid the instability of other replication scheme doi 101145 233269233330 synchronous replication deadlock lock replica sync	SIGMOD_Conference	
598719	jim_gray gianfranco_r._putzolu	the 5 minute rule for trading memory for disk access and the 10 byte rule for trading memory for cpu time	if a item be access frequently enough it should be main memory resident for current technology 8220 frequently enough 8221 mean about every five minute along a similar vein one can frequently trade memory space for cpu time for example bit can be pack in a byte at the expense of extra instruction to extract the bit it make economic sense to spend ten byte of main memory to save one instruction per second these result depend on current price ratio of processor memory and disc access these ratio be change and hence the constant in the rule be change doi 101145 3871338755 io memory resident reminiscent byte main memory disk	SIGMOD_Conference	
598721	jim_gray prakash_sundaresan susanne_englert kenneth_baclawski peter_j._weinberger	quickly generating billionrecord synthetic database	evaluate database system performance often require generate synthetic database 8212 one have certain statistical property but fill with dummy information when evaluate different database design it be often necessary to generate several database and evaluate each design as database size grow to terabyte generation often take longer than evaluation this paper present several database generation technique in particular it discuss lrb 1 rrb parallelism to get generation speedup and scaleup lrb 2 rrb congruential generator to get dense unique uniform distribution lrb 3 rrb specialcase discrete logarithm to generate index concurrent to the base table generation lrb 4 rrb modification of lrb 2 rrb to get exponential normal and selfsimilar distribution the discussion be in term of generate billionrecord sql database use c program run on a sharednothing computer system consist of a hundred processor with a thousand disc the idea apply to smaller database but large database present the more difficult problem doi 101145 191839191886 large database specific property test data synthetic data zipf distribution	SIGMOD_Conference	
598723	jim_gray hans-jorg_schek michael_stonebraker jeffrey_d._ullman	the lowell report	begin in 1989 a adhoc collection of senior dbm researcher have gather periodically to perform a group grope ie a assessment of the state of the art in dbms research as well as a prediction concern what problem and problem area deserve additional focus the fifth adhoc meeting be hold may 46 2003 in lowell ma a report on the meeting be in preparation and this panel discussion will summarize the upcome document and discuss its conclusion permission to make digital or hard copy of all or part of this work forr personal or classroom use be grant without fee provide that copy aree not make or distribute for profit or commercial advantage and that copiess bear this notice and the full citation on the first page to copy otherwise too republish to post on server or to redistribute to list require prior specificc permission andor a fee doi 101145 872757872873 meet	SIGMOD_Conference	
599440	chris_nyberg tom_barclay zarka_cvetanovic jim_gray david_b._lomet	alphasort a risc machine sort		SIGMOD_Conference	
599654	betty_salzberg alex_tsukerman jim_gray michael_stewart susan_uren bonnie_vaughan	fastsort a distribute singleinput singleoutput external sort	external singleinput singleoutput sort can use multiple processor each with a large tournament replacementselection in memory and each with private disk to sort a input stream in linear elapsed time of course increase number of processor memory and disk be require as the input file size grow this paper analyze the algorithm and report the performance of a implementation doi 101145 9359798719 disk	SIGMOD_Conference	
599773	alexander_s._szalay peter_z._kunszt ani_thakar jim_gray donald_r._slutz robert_j._brunner	designing and mining multiterabyte astronomy archives the sloan digital sky survey	the nextgeneration astronomy digital archive will cover most of the sky at fine resolution in many wavelength from xray through ultraviolet optical and infrare the archive will be store at diverse geographical location one of the first of these project the sloan digital sky survey lrb sdss rrb be create a 5wavelength catalog over 10000 square degree of the sky lrb see httpwwwsdssorg rrb the 200 million object in the multiterabyte database will have mostly numerical attribute in a 100 dimensional space point in this space have highly correlate distribution the archive will enable astronomer to explore the datum interactively datum access will be aid by multidimensional spatial and attribute index the datum will be partition in many way small i tag i object consist of the most popular attribute will accelerate frequent search split the datum among multiple server will allow parallel scalable io and parallel data analysis hash technique will allow efficient clustering and pairwise comparison algorithm that should parallelize nicely randomly sample subset will allow debugging otherwise large query at the desktop central server will operate a data pump to support sweep search touch most of the datum the anticipated query will require special operator relate to angular distance and complex similarity test of object property like shape color velocity vector or temporal behavior these issue pose interesting datum management challenge doi 101145 342009335439 sky quadtree archive astronomer sdss	SIGMOD_Conference	
599775	alexander_s._szalay jim_gray ani_thakar peter_z._kunszt tanu_malik jordan_raddick christopher_stoughton jan_vandenberg	the sdss skyserver public access to the sloan digital sky server datum	the skyserver provide internet access to the public sloan digital sky survey lrb sdss rrb datum for both astronomer and for science education this paper describe the skyserver goal and architecture it also describe we experience operate the skyserver on the internet the sdss datum be public and welldocumented so it make a good test platform for research on database algorithm and performance doi 101145 564691564758 federation pipeline skyserver astronomy astronomer sdss	SIGMOD_Conference	
599864	michael_stonebraker lawrence_a._rowe bruce_g._lindsay jim_gray michael_j._carey david_beech	the committee for advanced dbms function third generation data base system manifesto		SIGMOD_Conference	
617014	jim_gray	why do computer stop and what can be done about it	a analysis of the failure statistics of a commercially available faulttolerant system show that administration and software be the major contributor to failure various approach to software faulttolerance be then discussednotably processpair transaction and reliable storage it be point out that fault in production software be often soft lrb transient rrb and that a transaction mechanism combine with persistent processpair provide faulttolerant executionthe key to software faulttolerance disclaimer this paper be not a official tandem statement on faulttolerance rather it express the author s research on the topic ure heisenbug late software fault	Symposium_on_Reliability_in_Distributed_Software_and_Database_Systems	
617990	jim_gray	where the rubber meet the sky the semantic gap between datum producer and datum consumer		SSDBM	Microsoft
642043	dina_bitton jim_gray	disk shadow		VLDB	
642677	jim_gray	the transaction concept virtue and limitations lrb invited paper rrb		VLDB	
642678	jim_gray bob_horst mark_walker	parity striping of disk array lowcost reliable storage with acceptable throughput	a analysis of mirror disc and of raid5 show that mirror have considerably better throughput measure as requestssecond on random request of arbitrary size lrb up to 1mb rrb mirror have comparable or better response time for request of reasonable size lrb less than 100kb rrb but mirror have a 100 storage penalty store the datum twice parity striping be a data layout that stripe the parity across the disc but do not stripe the datum parity striping have throughput almost as good as mirror and have costgb comparable to raid5 designscombing the advantage of both for hightraffic disc resident datum parity striping have additional fault containment and software benefit as well parity striping sacrifice the high datum transfer rate of raid design for high throughput it be argue that response time and throughput be preferable performance metric	VLDB	
642679	jim_gray raymond_a._lorie gianfranco_r._putzolu irving_l._traiger	granularity of locks in a large shared data base	this paper propose a lock protocol which associate lock with set of resource this protocol allow simultaneous lock at various granularity by different transaction it be base on the introduction of additional lock mode besides the conventional share mode and exclusive mode the protocol be generalize from simple hierarchy of lock to direct acyclic graph of lock and to dynamic graph of lock the issue of scheduling and grant conflict request for the same resource be then discuss lastly these idea be compare with the lock mechanism provide by exist datum management system doi 101145 12824801282513	VLDB	
643455	banu_ozden eran_gabber bruce_hillyer wee_teck_ng elizabeth_a._m._shriver david_j._dewitt bruce_gordon jim_gray john_wilkes	storage service providers a solution for storage management lrb panel rrb		VLDB	
643587	andreas_reuter stefano_ceri jim_gray betty_salzberg gerhard_weikum	database and workflow management what be it all about lrb panel rrb		VLDB	
644107	hansjorg_zeller jim_gray	a adaptive hash join algorithm for multiuser environment	as main memory become a cheaper resource hash join be a alternative to the traditional method of perform equijoin nested loop and merge join this paper introduce a modify adaptive hash join method that be design to work with dynamic change in the amount of available memory the general idea of the algorithm be to regulate resource usage of a hash join in a way that allow it to run concurrently with other application the algorithm provide good performance for a broad range of problem size allow to join large table in a small main memory and use advanced io controller with tracksize io transfer it have be implement as a prototype in nonstop sql a dbms run on tandem machine rithm square root internal memory hash main memory	VLDB	
644283	jim_gray svein-olaf_hvasshovd	panel session do the dbms sw vendors offer the products require by the industrial user in the communication industry		Databases_in_Telecommunications	
654208	jim_gray alexander_s._szalay ani_thakar peter_z._kunszt christopher_stoughton donald_r._slutz jan_van_den_berg	data mining the sdss skyserver database	a earlier paper lrb szalay et al designing and mining multiterabyte astronomy archives the sloan digital sky survey acm sigmod 2000 rrb describe the sloan digital sky survey s lrb sdss rrb datum management need by define twenty database query and twelve datum visualization task that a good datum management system should support we build a database and interface to support both the query load and also a website for adhoc access this paper report on the database design describe the datum load pipeline and report on the query implementation and performance the query typically translate to a single sql statement most query run in less than 20 seconds allow scientist to interactively explore the database this paper be a indepth tour of those query reader should first have study the companion overview paper szalay et al the sdss skyserver public access to the sloan digital sky server datum acm sigmond 2002 pipeline astronomy schema sdss sql	WDAS	
769438	serge_abiteboul rakesh_agrawal philip_a._bernstein michael_j._carey stefano_ceri w._bruce_croft david_j._dewitt michael_j._franklin hector_garcia-molina dieter_gawlick jim_gray laura_m._haas alon_y._halevy joseph_m._hellerstein yannis_e._ioannidis martin_l._kersten michael_j._pazzani michael_lesk david_maier jeffrey_f._naughton hans-jorg_schek timos_k._sellis avi_silberschatz michael_stonebraker richard_t._snodgrass jeffrey_d._ullman gerhard_weikum jennifer_widom stanley_b._zdonik	the lowell database research selfassessment	database need be change drive by the internet and increase amount of scientific and sensor datum in this article the author propose research into several important new direction for database management system doi 101145 10607101060718 database research fuzzy logic globalization eg sql	Commun._ACM	
769921	gordon_bell jim_gray	digital immortality	1 this work have be submit for publication to the communications of the acm copyright may be transfer without further notice and the publisher may then post the accept version doi 101145 365181365182	Commun._ACM	Microsoft Redmond WA
769922	gordon_bell jim_gray	what be next in highperformance computing	we can trace the evolution from crays to cluster to supercompute center but where do it go from here doi 101145 503124503129 fault tolerance total cost port hpc single	Commun._ACM	Bay_Area_Research_Center_of_Microsoft_Research San_Francisco CA
770550	donald_d._chamberlin morton_m._astrahan mike_w._blasgen jim_gray w._frank_king_iii bruce_g._lindsay raymond_a._lorie james_w._mehl thomas_g._price gianfranco_r._putzolu patricia_g._selinger mario_schkolnick donald_r._slutz irving_l._traiger bradford_w._wade robert_a._yost	a history and evaluation of system r	system r a experimental database system be construct to demonstrate that the usability advantage of the relational datum model can be realize in a system with the complete function and high performance require for everyday production use this paper describe the three principal phase of the system r project and discuss some of the lesson learn from system r about the design of relational system and database system in general doi 101145 358769358784	Commun._ACM	
771176	david_j._dewitt jim_gray	parallel database systems the future of high performance database systems	the success of these system refute a 1983 paper predict the demise of database machine lsb 3 rsb ten year ago the future of highly parallel database machine seem gloomy even to they staunchest advocate most database machine research have focus on specialize often trendy hardware such as ccd memory bubble memory headpertrack disk and optical disk none of these technology fulfil they promise so there be a sense that conventional cpus electronic ram and mcvinghead magnetic disk would dominate the scene for many year to come at that time disk throughput be predict to double while processor speed be predict to increase by much larger factor consequently critic predict that multiprocessor system would scxm be io limit unless a solution to the io bottleneck be find whiie these prediction be fairly accurate about the future of hardware the critic be certainly wrong about the overall future of parallel database system over the last decade eradata tandem and a host of startup company have successfully develop and market highly parallel machine doi 101145 129888129894 database systems sharednothing dbm disk ture	Commun._ACM	
771521	kapali_p._eswaran jim_gray raymond_a._lorie irving_l._traiger	the notion of consistency and predicate lock in a database system	in database system user access share datum under the assumption that the datum satisfy certain consistency constraint this paper define the concept of transaction consistency and schedule and show that consistency require that a transaction can not request new lock after release a lock then it be argue that a transaction need to lock a logical rather than a physical subset of the database these subset may be specify by predicate a implementation of predicate lock which satisfy the consistency condition be suggest doi 101145 360363360369 concurrency twophase serializability phase lock 2pl	Commun._ACM	
772375	jim_gray alexander_s._szalay	the worldwide telescope	mining vast database of astronomical datum this new online way to see the global structure of the universe promise to be not only a wonderful virtual telescope but a archetype for the evolution of computational science doi 101145 581571581572 universe caching archive portal parallel computer	Commun._ACM	Microsoft_Research_Group San_Francisco CA
805711	morton_m._astrahan mike_w._blasgen donald_d._chamberlin jim_gray w._frank_king_iii bruce_g._lindsay raymond_a._lorie james_w._mehl thomas_g._price gianfranco_r._putzolu mario_schkolnick patricia_g._selinger donald_r._slutz h._raymond_strong paolo_tiberio irving_l._traiger bradford_w._wade robert_a._yost	system r a relational data base management system	a relational approach make this experimental datum base management system unusually easy to install and use some of the decision make in system r design in order to enhance usability also offer major bonus in other area doi 101109 mc 19791658743	IEEE_Computer	IBM_San_Jose_Research_Laboratory
805782	gordon_bell jim_gray alexander_s._szalay	petascale computational systems	a balanced cyberinfrastructure be necessary to meet grow dataintensitive scientific need we believe that available resource should be allocate to benefit the broadest crosssection of the scientific community give the powerlaw distribution of problem size this mean that about half of funding agency resource should be spend on tier1 center at the petascale level and the other half dedicate to tier2 and tier3 center on a costsharing basis funding agency should support balanced system not just cpu farm as well as petascale io and networking they should also allocate resource for a balanced tier1 through tier3 cyberinfrastructure doi 101109 mc 200629 balanced flood byte empirical theoretical	IEEE_Computer	Microsoft_Research
806566	jim_gray	evolution of data management	objective alternate hemiplegia of childhood be a predominantly sporadic neurodevelopmental syndrome of uncertain etiology in more than 3 decade since its description little progress have be make in understand its etiology or in identify effective treatment in 1998 in collaboration with the alternating hemiplegia of childhood foundation a international registry be establish to help document clinical outcome and promote research effort patient and method we present phenotypic datum on 103 patient who meet exist diagnostic criterion for alternate hemiplegia of childhood although some of these subject may have be include in previously publish review we focus be direct toward the earliest manifestation of symptom and evolution of feature over time datum source include write questionnaire facetoface and telephone interview clinical examination and medical chart characteristic of disease onset medical comorbidity episode trigger diagnostic workup and treatment be present result paroxysmal eye movement be the most frequent early symptom manifest in the first 3 month of life in 83 of patient hemiplegic episode appear by 6 month of age in 56 of infant background slow show by electroencephalography during typical paroxysmal event include hemiplegic tonic or dystonic episode be frequent lrb 21 of 42 case rrb distinct convulsive episode with altered consciousness believe to be epileptic in nature be report in 41 of patient ataxia lrb 96 rrb and cognitive impairment lrb 100 rrb be frequent nonepisodic symptom empiric pharmacologic treatment approach offer little benefit in most subject and result in adverse effect in 20 of patient prolonged episode be completely or temporarily abort during sleep in all subject conclusion this descriptive analysis of a large cohort of child indicate that paroxysmal ocular movement be a early highly suggestive symptom follow by paroxysmal episode of focal dystonia or flaccid alternate hemiplegia in early infancy in the majority of subject current challenge in diagnosis and management contribute to poor outcome early diagnosis and multicenter collaboration be need to facilitate trial to identify more effective therapy doi 101542 peds20082027	IEEE_Computer	Microsoft_Res. San_Francisco CA USA
806569	jim_gray daniel_p._siewiorek	highavailability computer systems	today s highly available system deliver four year of uninterrupted service the challenge be to build system with 100year mean time to failure and oneminute repair time september 1991 aradoxically the larger a system be the more criticalbut less likelyit be to be highly available we can build small ultraavailable module but build large system involve thousand of module and million of line of code be a poorly understand art even though such large system be a core technology of modern society three decade ago hardware component be the major source of fault and outage today hardware fault be a relatively minor cause of system outage when compare with operation environment and software fault technique and design that tolerate these broader class of fault be still in they infancy this article sketch the technique use to build highly available computer system computer build in the late 1950s offer a 12hour mean time to failure a maintenance staff of a dozen fulltime computer engineer could repair the machine in about eight hour this failurerepair cycle provide 60 percent availability the vacuum tube and relay component of these computer be the major source of failure they have lifetime of a few month so the machine rarely operate for more than a day without interruption many faultdetection and faultmasking technique use today be first use on these early computer diagnostics test the machine selfchecking computational technique detect fault while the computation progress the program occasionally save lrb checkpoint rrb its state on stable media after a failure and repair the program read the most recent checkpoint and continue the computation from that point this checkpointrestart technique let computer that fail every few hour perform longrunning computation device improvement have increase computer system availability by 1980 typical wellrun computer system offer 99 percent availability 2 this sound good but 99 percent availability be 100 minute of downtime per week such computer systems large systems major source	IEEE_Computer	Digital_Equipment_Corp. San_Francisco CA
816317	jim_gray	locally serve network computers	nc be the natural evolution of pc ubiquitous computer everywhere the current vision of ncs require two improbable development lrb 1 rrb inexpensive highbandwidth wan link to the internet and lrb 2 rrb inexpensive centralized server the large nc bandwidth requirement will force each home or office to have a local server lan attach to the nc these server will be much less expensive to purchase and manage than a centralized solution centralized staff be expensive and unresponsive ncs manageability several time management cost	CoRR	
816319	bill_devlin jim_gray bill_laing george_spix	scalability terminology farms clone partitions packs rac and rap	define a vocabulary for scaleable system geoplex farms clone rac rap clone partition and pack and dicuss the design tradeoff of use clone partiton and pack rac rap clone pack	CoRR	
817465	jim_gray alexander_s._szalay ani_thakar peter_z._kunszt christopher_stoughton donald_r._slutz jan_vandenberg	data mining the sdss skyserver database	a earlier paper lrb szalay et al designing and mining multiterabyte astronomy archives the sloan digital sky survey acm sigmod 2000 rrb describe the sloan digital sky survey s lrb sdss rrb datum management need by define twenty database query and twelve datum visualization task that a good datum management system should support we build a database and interface to support both the query load and also a website for adhoc access this paper report on the database design describe the datum load pipeline and report on the query implementation and performance the query typically translate to a single sql statement most query run in less than 20 seconds allow scientist to interactively explore the database this paper be a indepth tour of those query reader should first have study the companion overview paper szalay et al the sdss skyserver public access to the sloan digital sky server datum acm sigmond 2002 pipeline astronomy schema sdss sql	CoRR	
817478	alexander_s._szalay jim_gray jan_vandenberg	petabyte scale data mining dream or reality	science be become very datum intensive 1 today s astronomy dataset with ten of million of galaxy already present substantial challenge for datum mining in less than 10 year the catalog be expect to grow to billion of object and image archive will reach petabytes imagine have a 100gb database in 1996 when disk scanning speed be 30mbs and database tool be immature such a task today be trivial almost manageable with a laptop we think that the issue of a pb database will be very similar in six year in this paper we scale we current experiment in datum archiving and analysis on the sloan digital sky survey 23 datum six year into the future we analyze these projection and look at the requirement of perform datum mining on such datum set we conclude that the task scale rather well we could do the job today although it would be expensive there do not seem to be any showstopper that would prevent we from store and use a petabyte dataset six year from today 1 collecting petabyte datasets 11 generating a petabyte there be several concrete experiment lrb lsst 4 panstarrs 5 rrb which be aim at such datum set instead of take the precise parameter of either we will try do define a idealized experiment that be in the same ballpark with the approximate capability of generate a petabyte of imaging datum per year we be work backwards from a 1 petabyteyear datum size assume pixel size of about 025 the whole sky be about 10 terapixel we take this we canonical observation this can be split as half the sky in two band or quarter of the sky in four band etc all yield about 20tb of raw imaging datum consider 2 byte per pixel it be unlikely that we will attempt to image the sky in 50 band thus we assume that the primary aim of the experiment be to get timedomain datum detect transient and move object in a small number of filter a image from a 5 gigapixel camera be 10 gbyte take a series of 1 minute exposure translate to about 5 tbnightgpixels the propose experiment plan on cover the sky in about 4 night this translate to about 5tbnight when the weather be good enough for imaging a consistent number we also assume a excellent site with 200 night of observation per doi 101117 12461427 sky petabyte data mining data per year night astronomy byte	CoRR	
817479	alexander_s._szalay tamas_budavari andrew_connolly jim_gray takahiko_matsubara adrian_pope istvan_szapudi	spatial clustering of galaxy in large dataset	dataset with ten of million of galaxy present new challenge for the analysis of spatial clustering we have build a framework that integrate a database of object catalog tool for create mask of bad region and a fast lrb nlogn rrb correlation code this system have enable unprecedented efficiency in carry out the analysis of galaxy clustering in the sdss catalog a similar approach be use to compute the threedimensional spatial clustering of galaxy on very large scale we describe we strategy to estimate the effect of photometric error use a database we discuss we effort as a early example of dataintensive science while it would have be possible to get these result without the framework we describe it will be infeasible to perform these computation on the future huge dataset without use this framework doi 101117 12476761 sky galaxy universe speedup sdss	CoRR	
817512	serge_abiteboul rakesh_agrawal philip_a._bernstein michael_j._carey stefano_ceri w._bruce_croft david_j._dewitt michael_j._franklin hector_garcia-molina dieter_gawlick jim_gray laura_m._haas alon_y._halevy joseph_m._hellerstein yannis_e._ioannidis martin_l._kersten michael_j._pazzani michael_lesk david_maier jeffrey_f._naughton hans-jorg_schek timos_k._sellis avi_silberschatz michael_stonebraker richard_t._snodgrass jeffrey_d._ullman gerhard_weikum jennifer_widom stanley_b._zdonik	the lowell database research self assessment	database need be change drive by the internet and increase amount of scientific and sensor datum in this article the author propose research into several important new direction for database management system doi 101145 10607101060718 database research fuzzy logic globalization eg sql	CoRR	
817535	maria_a._nieto-santisteban william_o'mullane jim_gray nolan_li tamas_budavari alexander_s._szalay aniruddha_r._thakar	extend the sdss batch query system to the national virtual observatory grid	the sloan digital sky survey science database be approach 2tb while the vast majority of query normally execute in seconds or minute this interactive execution time can be disproportionately increase by a small fraction of query that take hour or day to run either because they require nonindex scan of the largest table or because they request very large result set in response to this we add a multiqueue job submission and tracking system the transfer of very large result set from query over the network be another serious problem statistics suggest that much of this datum transfer be unnecessary user would prefer to store result locally in order to allow further cross matching and filter to allow local analysis we implement a system that give user they own personal database lrb mydb rrb at the portal site user may transfer datum to they mydb and then perform further analysis before extract it to they own machine we intend to extend the mydb and asynchronous query idea to multiple nvo node this imply development in a distribute manner of several feature which have be demonstrate for a single node in the sdss batch query system lrb casjobs rrb the generalization of asynchronous query necessitate some form of mydb storage as well as workflow tracking service on each node and coordination strategy among node large result set	CoRR	
817536	jim_gray alexander_s._szalay	the world wide telescope a archetype for online science	most scientific datum will never be directly examine by scientist rather it will be put into online database where it will be analyze and summarize by computer program scientist increasingly see they instrument through online scientific archive and analysis tool rather than examine the raw datum today this analysis be primarily drive by scientist ask query but scientific archive be become active database that selforganize and recognize interesting and anomalous fact as datum arrive in some field datum from many different archive can be crosscorrelate to produce new insight astronomy present a excellent example of these trend and f ederating astronomy archive present interesting challenge for computer scientist introduction computational science be a new branch of most discipline a thousand year ago science be primarily empirical over the last 500 year each discipline have grow a theoretical component theoretical model often motivate experiment and generalize we understanding today most discipline have both empirical and theoretical branch in the last 50 year most discipline have grow a third computational branch lrb eg empirical theoretical and computational ecology or physics or linguistics rrb archive empirical theoretical	CoRR	
817537	aniruddha_r._thakar alexander_s._szalay peter_z._kunszt jim_gray	the sloan digital sky survey science archive migrate a multiterabyte astronomical archive from object to relational dbms	a ab bs st tr ra ac ct t the sloan digital sky survey science archive be the first in a series of multiterabyte digital archive in astronomy and other dataintensive science to facilitate datum mining in the sdss archive we adapt a commercial database engine and build specialized tool on top of it originally we choose a objectoriented database management system due to its datum organization capability platform independence query performance and conceptual fit to the datum however after use the object database for the first couple of year of the project it soon begin to fall short in term of its query support and datum mining performance this be as much due to the inability of the database vendor to respond we demand for feature and bug fix as it be due to they failure to keep up with the rapid improvement in hardware performance particularly faster raid disk system in the end we be force to abandon the object database and migrate we datum to a relational database we describe below the technical issue that we face with the object database and how and why we migrate to relational technology 1 1 i in nt tr ro od du uc ct ti io on n the advent of digital archive enable by quantum leap in the technology to publish distribute and mine datum over the internet have give rise to a data avalanche in many branch of science and engineering the human genome project and the large hadron collider be two example of very large scientific dataset come online in the biological and particle physics community respectively astronomy be no exception and be perhaps more deluge by a flood of new datum from current and proposed sky survey than any other science in this age of multiterabyte scientific archive scientist need to share the common lesson from different discipline to make the most of the opportunity available and to avoid be overwhelm by the datum avalanche the sloan digital sky survey lrb sdss rrb be a multiinstitution project to map about half of the northern sky in five wavelength band from ultraviolet to infrare lrb see httpwwwsdssorg rrb when complete lrb 2005 rrb the survey be expect to image over 200 million object and collect spectrum lrb redshift rrb for the brightest 1 million galaxy among these the sdss will revolutionize astronomy in a number of way but most significantly it will dwarf current astronomical digital archive data avalanche data mining object database sdss	CoRR	
817538	tom_barclay wyman_chong jim_gray	a quick look at sata disk performance	we have be investigate the use of lowcost commodity component for multiterabyte sql server database dub storage brick these server be white box pc contain the largest ata drive valuepriced amd or intel processor and inexpensive ecc memory one issue have be the wiring mess air flow problem length restriction and connector failure create by seven or more parallel ata lrb pata rrb ribbon cable and drive in rsb a tower or 3u rackmount chassis large capacity serial ata lrb sata rrb drive have recently become widely available for the pc environment at a reasonable price in addition to be faster the sata connector seem more reliable have a more reasonable length restriction lrb 1m rrb and allow better airflow we test two drive brand along with two raid controller to evaluate sata drive performance and reliablility this paper document we result so far drive	CoRR	
817550	jim_gray	the revolution in database system architecture	database system architecture be undergo revolutionary change most importantly algorithm and datum be be unify by integrate programming language with the database system this give a extensible objectrelational system where nonprocedural relational operator manipulate object set couple with this each dbms be now a web service this have huge implication for how we structure application dbmss be now object container queue be the first object to be add these queue be the basis for transaction processing and workflow application future workflow system be likely to be build on this core datum cube and online analytic processing be now baked into most dbmss beyond that dbmss have a framework for datum mining and machine learning algorithm decision tree bayes net clustering and time series analysis be build in new algorithm can be add there be a rebirth of column store for sparse table and to optimize bandwidth text temporal and spatial datum access method along with they probabilistic reasoning have be add to database system allow approximate and probabilistic answer be essential for many application many believe that xml and xquery will be the main datum structure and access pattern database system must accommodate that perspective external datum increasingly arrive as stream to be compare to historical datum so streamprocessing operator be be add to the dbms publishsubscribe system invert the dataquery ratio incoming datum be compare against million of query rather than query search million of record meanwhile disk and memory capacity be grow much faster than they bandwidth and latency so the database system increasingly use huge main memory and sequential disk access these change mandate a much more dynamic query optimization strategy one that adapt to current condition and selectivity rather than have a static plan intelligence be move to the periphery of the network each disk and each sensor will be a competent database machine relational algebra be a convenient way to program these system database system be now expect to be selfmanaging selfhealing and alwaysup we researcher and developer have we work cut out for we in deliver all these feature dbm disk dbmss database systems	CoRR	
817551	jim_gray alexander_s._szalay aniruddha_r._thakar gyorgy_fekete william_o'mullane maria_a._nieto-santisteban gerd_heber arnold_h._rots	there go the neighborhood relational algebra for spatial data search	we explore way of do spatial search within a relational database lrb 1 rrb hierarchical triangular mesh lrb a tessellation of the sphere rrb lrb 2 rrb a zoned bucketing system and lrb 3 rrb represent area as disjunctivenormal form constraint each of these approach have merit they all allow efficient pointinregion query a relational representation for region allow boolean operation among they and allow quick test for pointinregion regionscontainingpoint and regionoverlap the speed of these algorithm be much improve by a zone and multiscale zonepyramid scheme the approach have the virtue that the zone mechanism work well on btree native to all sql system and integrate naturally with current query optimizer rather than require a new spatial access method and concomitant query optimizer extension over the last 5 year we have use these technique extensively in we work on skyserversdssorg and skyquerynet zone	CoRR	
817560	jim_gray joshua_coates chris_nyberg	performance price sort		CoRR	
817561	jim_gray goetz_graefe	the fiveminute rule ten year later and other computer storage rule of thumb	simple economic and performance argument suggest appropriate lifetime for main memory page and suggest optimal page size the fundamental tradeoff be the price and bandwidth of ram and disk the analysis indicate that with today s technology five minute be a good lifetime for randomly access page one minute be a good lifetime for twopass sequentially access page and 16 kb be a good size for index page these rulesofthumb change in predictable way as technology ratio change they also motivate the importance of the new italic kaps maps scan italic and italic kaps map tbscan italic metric doi 101145 271074271094 ram physical characteristic main memory extra block size memory hierarchy rent disk chart	CoRR	
817562	tom_barclay robert_eberl jim_gray john_nordlinger guru_raghavendran donald_r._slutz greg_smith phil_smoot john_hoffman natt_robb_iii hedy_rossmeissl beth_duff george_lee theresa_mathesmier randall_sunne	microsoft terraserver	the	CoRR	
817565	philip_a._bernstein michael_l._brodie stefano_ceri david_j._dewitt michael_j._franklin hector_garcia-molina jim_gray gerald_held joseph_m._hellerstein h._v._jagadish michael_lesk david_maier jeffrey_f._naughton hamid_pirahesh michael_stonebraker jeffrey_d._ullman	the asilomar report on database research	the database research community be rightly proud of success in basic research and its remarkable record of technology transfer now the field need to radically broaden its research focus to attack the issue of capture store analyze and present the vast array of online datum the database research community should embrace a broader research agenda 8212 broaden the definition of database management to embrace all the content of the web and other online datum store and rethink we fundamental assumption in light of technology shift to accelerate this transition we recommend change the way research result be evaluate and present in particular we advocate encourage more speculative and longrange work move conference to a poster format and publish all research literature on the web doi 101145 306101306137 database research xml asilomar report database community main memory	CoRR	
817567	alexander_s._szalay peter_z._kunszt ani_thakar jim_gray	designing and mining multiterabyte astronomy archives the sloan digital sky survey	the nextgeneration astronomy digital archive will cover most of the sky at fine resolution in many wavelength from xray through ultraviolet optical and infrare the archive will be store at diverse geographical location one of the first of these project the sloan digital sky survey lrb sdss rrb be create a 5wavelength catalog over 10000 square degree of the sky lrb see httpwwwsdssorg rrb the 200 million object in the multiterabyte database will have mostly numerical attribute in a 100 dimensional space point in this space have highly correlate distribution the archive will enable astronomer to explore the datum interactively datum access will be aid by multidimensional spatial and attribute index the datum will be partition in many way small i tag i object consist of the most popular attribute will accelerate frequent search split the datum among multiple server will allow parallel scalable io and parallel data analysis hash technique will allow efficient clustering and pairwise comparison algorithm that should parallelize nicely randomly sample subset will allow debugging otherwise large query at the desktop central server will operate a data pump to support sweep search touch most of the datum the anticipated query will require special operator relate to angular distance and complex similarity test of object property like shape color velocity vector or temporal behavior these issue pose interesting datum management challenge doi 101145 342009335439 sky quadtree archive astronomer sdss	CoRR	
817568	tom_barclay jim_gray donald_r._slutz	microsoft terraserver a spatial data warehouse	the terraserver store aerial satellite and topographic image of the earth in a sql database available via the internet it be the world s largest online atla combine five terabyte of image datum from the united states geological survey lrb usgs rrb and spin2 this report describe the systemredesign base on we experience over the last year it also report usage and operation result over the last year over 2 billion web hit and over 20 terabyte of imagry serve over the internet internet browser provide intuitive spatial and text interface to the datum user need no special hardware software or knowledge to locate and browse imagery this paper describe how terabyte of internet unfriendly geospatial image be scrub and edit into hundred of million of internet friendly image tile and load into a sql datum warehouse microsoft terraserver demonstrate that generalpurpose relational database technology can manage large scale image repository and show that web browser can be a good geospatial image presentation system satellite image datum sql database terabyte imagery	CoRR	
817635	alexander_s._szalay tamas_budavari tanu_malik jim_gray ani_thakar	web services for the virtual observatory	web services form a new emerge paradigm to handle distribute access to resource over the internet there be platform independent standard lrb soap wsdl rrb which make the developer task considerably easier this article dis cuss how web service could be use in the context of the virtual observatory we envisage a multilayer architecture with interoperate service a welldesigned lower layer consist of simple standard service implement by most datum provider will go a long way towards establish a modular architecture more complex application can be build upon this core layer we present two prototype application the sdsscutout and the skyquery as example of this layered architecture moore s law same technology	CoRR	
817774	jim_gray leslie_lamport	consensus on transaction commit	the distribute transaction commit problem require reach agreement on whether a transaction be commit or abort the classic twophase commit protocol block if the coordinator fail faulttolerant consensus algorithm also reach agreement but do not block whenever any majority of the process be work the paxos commit algorithm run a paxos consensus algorithm on the commitabort decision of each participant to obtain a transaction commit protocol that use 2 i f i plus 1 coordinator and make progress if at least i f i plus 1 of they be work properly paxos commit have the same stablestorage write delay and can be implement to have the same message delay in the faultfree case as twophase commit but it use more message the classic twophase commit algorithm be obtain as the special i f i equal 0 case of the paxos commit algorithm doi 101145 11328631132867 2pc locking leader paxos coordinator	CoRR	
817810	alexander_s._szalay jim_gray ani_thakar peter_z._kunszt tanu_malik jordan_raddick christopher_stoughton jan_vandenberg	the sdss skyserver public access to the sloan digital sky server datum	the skyserver provide internet access to the public sloan digital sky survey lrb sdss rrb datum for both astronomer and for science education this paper describe the skyserver goal and architecture it also describe we experience operate the skyserver on the internet the sdss datum be public and welldocumented so it make a good test platform for research on database algorithm and performance doi 101145 564691564758 federation pipeline skyserver astronomy astronomer sdss	CoRR	
817814	alexander_s._szalay jim_gray ani_thakar peter_z._kunszt tanu_malik jordan_raddick christopher_stoughton jan_vandenberg	the sdss skyserver public access to the sloan digital sky server datum	the skyserver provide internet access to the public sloan digital sky survey lrb sdss rrb datum for both astronomer and for science education this paper describe the skyserver goal and architecture it also describe we experience operate the skyserver on the internet the sdss datum be public and welldocumented so it make a good test platform for research on database algorithm and performance doi 101145 564691564758 federation pipeline skyserver astronomy astronomer sdss	CoRR	
817816	tom_barclay jim_gray eric_strand steve_ekblad jeffrey_richter	terraservicenet a introduction to web services	this article explore the design and construction of a geospatial internet web service application from the host web site perspective and from the perspective of a application use the web service the terraservicenet web service be add to the popular terraserver database and web site with no major structural change to the database the article discuss web service design implementation and deployment concept and design guideline web service enable application that aggregate and interact with information and resource from internetscale distribute server the article present the design of two usda application that interoperate with database and web service resource in fort collins colorado and the terraservice web service located in tukwila washington web site satellite	CoRR	
817817	jim_gray alexander_s._szalay ani_thakar christopher_stoughton jan_vandenberg	online scientific data curation publication and archiving	science project be datum publisher the scale and complexity of current and future science datum change the nature of the publication process publication be become a major project component at a minimum a project must preserve the ephemeral datum it gather derived datum can be reconstruct from metadata but metadata be ephemeral longer term a project should expect some archive to preserve the datum we observe that publish scientific datum need to be available forever this give rise to the datum pyramid of version and to datum inflation where the derive datum volume explode as a example this article describe the sloan digital sky survey lrb sdss rrb strategy for datum publication datum access curation and preservation 1 introduction once publish scientific datum should remain available forever so that other scientist can reproduce the result and do new science with the datum datum may be use long after the project that gather it end later user will not implicitly know the detail of how the datum be gather and prepare to understand the datum those later user need the metadata lrb 1 rrb how the instrument be design and build lrb 2 rrb when where and how the datum be gather and lrb 3 rrb a careful description of the processing step that lead to the derive data product that be typically use for scientific data analysis metadata processing step datum publication curation scientific data	CoRR	
818108	gordon_bell jim_gray	the revolution yet to happen	by 2047 almost all information will be in cyberspace lrb 1984 rrb include all knowledge and creative work all information about physical object include human building process and organization will be online this trend be both desirable and inevitable cyberspace will provide the basis for wonderful new way to inform entertain and educate people the information and the corresponding system will streamline commerce but will also provide new level of personal service health care and automation the most significant benefit will be a breakthrough in we ability to remotely communicate with one another use all we sens the acm and the transistor be bear in 1947 at that time the store program computer be a revolutionary idea and the transistor be just a curiosity both idea evolve rapidly by the mid 1960 integrate circuit appearedallowing mass fabrication of transistor on silicon substrate this allow lowcost massproduced computer these technology enable extraordinary increase in process speed and memory couple with extraordinary price decline the only form of processing and memory more easily cheaply and rapidly fabricate be the human brain peter cohrane lrb 1996 rrb estimate the brain to have a processing power of around 1000 millionmillion operation per second lrb one petaops rrb and a memory of 10 terabyte if current trend continue computer could have these capability by 2047 such computer could be on body personal assistant able to recall everything one read hear and see acm transistor	CoRR	
818110	jim_gray	what next a dozen informationtechnology research goals	charles babbage s vision of computing have largely be realize we be on the verge of realize vannevar bush s memex but we be some distance from pass the ture test these three vision and they associate problem have provide longrange research goal for many of we for example the scalability problem have motivate i for several decade this talk define a set of fundamental research problem that broaden the babbage bush and turing vision they extend babbage s computational goal to include highlysecure highlyavailable selfprogramming selfmanaging and selfreplicating system they extend bush s memex vision to include a system that automatically organize index digest evaluate and summarize information lrb as well as a human might rrb another group of problem extend turing s vision of intelligent machine to include prosthetic vision speech hearing and other sens each problem be simply state and each be orthogonal from the other though they share some common core technology doi 101145 602382602401 human expert	CoRR	
818825	jim_gray wyman_chong tom_barclay alexander_s._szalay jan_vandenberg	terascale sneakernet use inexpensive disk for backup archiving and data exchange	large dataset be most economically trnsmitt via parcel post give the current economics of widearea networking this article describe how the sloan digital sky survey ship terabyte scale dataset both within the us and to europe and asia we 3gt storage brick lrb ghz processor gb ram gbpsethernet tb disk rrb for about 2k each these brick act as database server on the lan they be load at one site and read at the second site the paper describe the brick they economics and some software issue that they raise drive mail brick tape large datum set	CoRR	
818865	jim_gray	distribute computing economics	computing economics be change today there be rough price parity between lrb 1 rrb one database access lrb 2 rrb 10 byte of network traffic lrb 3 rrb 100000 instruction lrb 4 rrb 10 byte of disk storage and lrb 5 rrb a megabyte of disk bandwidth this have implication for how one structure internetscale distribute computing one put computing as close to the datum as possible in order to avoid expensive network traffic doi 101145 13941271394131 ist tcp computing resource administrator byte	CoRR	
819855	jim_gray surajit_chaudhuri adam_bosworth andrew_layman don_reichart murali_venkatrao frank_pellow hamid_pirahesh	datum cube a relational aggregation operator generalize groupby crosstab and subtotal	data analysis application typically aggregate datum across many dimension look for anomaly or unusual pattern the sql aggregate function and the group by operator produce zerodimensional or onedimensional aggregate application need the ndimensional generalization of these operator this paper define that operator call the datum cube or simply cube the cube operator generalize the histogram crosstabulation rollup drilldown and subtotal construct find in most report writer the novelty be that cube be relation consequently the cube operator can be imbed in more complex nonprocedural data analysis program the cube operator treat each of the n aggregation attribute as a dimension of nspace the aggregate of a particular set of attribute value be a point in this space the set of point form a ndimensional cube superaggregate be compute by aggregate the ncube to lower dimensional space this paper lrb 1 rrb explain the cube and rollup operator lrb 2 rrb show how they fit in sql lrb 3 rrb explain how user can define new aggregate function for cube and lrb 4 rrb discuss efficient technique to compute the cube many of these feature be be add to the sql standard doi 101023 a 1009726021843 datum analysis groupby cube anomaly sql	CoRR	Microsoft_Corp. Redmond WA
819856	jim_gray	data management past present and future	sensor system typically operate under resource constraint that prevent the simultaneous use of all resource all of the time sensor management become relevant when the sense system have the capability of actively manage these resource ie change its operating configuration during deployment in reaction to previous measurement example of system in which sensor management be currently use or be likely to be use in the near future include autonomous robot surveillance and reconnaissance network and waveformagile radar this paper provide a overview of the theory algorithm and application of sensor management as it have develop over the past decade and as it stand today group testing sensor management sequential hypothesis testing pomdp broad spectrum	CoRR	
819857	hal_berenson philip_a._bernstein jim_gray jim_melton elizabeth_j._o'neil patrick_e._o'neil	a critique of ansi sql isolation level	ansi sql92 lsb ms ansi rsb define isolation i level i in term of i phenomenon i dirty read nonrepeatable read and phantoms this paper show that these phenomenon and the ansi sql definition fail to properly characterize several popular isolation level include the standard lock implementation of the level cover ambiguity in the statement of the phenomenon be investigate and a more formal statement be arrive at in addition new phenomenon that better characterize isolation type be introduce finally a important multiversion isolation type call snapshot isolation be define doi 101145 223784223785 ansi serializability isolation level snapshot isolation ple	CoRR	
819858	jim_gray	queue be database	messageorientedmiddleware lrb mom rrb have become a small industry mom offer queue transaction processing as a advance over pure clientserver transaction processing this note make four point queue transaction processing be less general than direct transaction processing queue system be build on top of direct system you can not build a direct system atop a queue system it be difficult to build direct conversational or distribute transaction atop a queue system queue be interesting database with interesting concurrency control it be best to build these mechanism into a standard database system so other application can use these interesting feature queue system need dbms functionality queue need security configuration performance monitoring recovery and reorganization utility database system already have these feature a fullfunction mom system duplicate these database feature queue manager be simple tpmonitor manage server pool drive by queue database system be encompass many server pool feature as they evolve to tplite system database systems mom direct queue transaction processing	CoRR	
819859	gerd_heber jim_gray	support finite element analysis with a relational database backend part i there be life beyond files	in this paper we show how to use a relational database management system in support of finite element analysis we believe it be a new way of think about datum management in wellunderstood application to prepare they for two major challengessize and integration lrb globalization rrb neither extreme size nor integration lrb with other application over the web rrb be a design concern 30 year ago when the paradigm for fea implementation first be form on the other hand database technology have come a long way since its inception and it be past time to highlight its usefulness to the field of scientific computing and computer base engineering this series aim to widen the list of application for database designer and for fea user and application developer to reap some of the benefit of database development data management fea finite element analysis microsoft sql server rdbm	CoRR	
819860	gerd_heber jim_gray	support finite element analysis with a relational database backend part ii database design and access	this be part ii of a three article series on use database for finite element analysis lrb fea rrb it discuss lrb 1 rrb db design lrb 2 rrb datum load lrb 3 rrb typical use case during grid building lrb 4 rrb typical use case during simulation lrb get and put rrb lrb 5 rrb typical use case during analysis lrb also do in part iii rrb and some performance measure of these case it argue that use a database be simpler to implement than custom datum schema have better performance because it can use datum parallelism and better support fea modularity and tool evolution because database schema evolution datum independence and selfdefining datum finite element analysis typical use case	CoRR	
819861	jim_gray charles_levine	thousand of debitcredit transactionspersecond easy and inexpensive	a 2k computer can execute about 8k transaction per second this be 80x more than one of the largest us bank s 1970 s traffic it approximate the total us 1970 s financial transaction volume very modest modern computer can easily solve yesterday s problem 1 a thousandtransactionspersecond be once difficult and expensive in 1973 bank of america want to convert they paper base branch teller and demanddeposit lrb savings rrb account to a online system let teller perform a customer s deposit and withdrawal the corresponding transaction profile call debitcredit evolve to become a standard measure of transaction processing lsb serlin rsb at the time the system of ten thousand teller need to perform 100 transaction per second the ten million account record be about 1gb and the 90day general ledger be about 4gb at the time the server hardware for such a system cost more than ten million dollar but it be not until 1976 that a commercial database system be able to run 100 transaction per second lsb gawlick rsb a decade later tandem use a 34cpu 86disk sql system cost ten million dollar to process 208transactions per second at the time this be consider a breakthrough because relational system have a reputation for poor performance lsb tandem rsb for much of the 1980 s the database and transaction processing performance agenda be to achieve a thousand transaction per second part of that process define the onetransaction per second unit informal definition lsb datamation rsb lsb 1ktp rsb and benchmarketing eventually lead to the formation of the transaction processing performance council lrb wwwtpcorg rrb which define the tpca transaction profile largely in line with debitcredit lsb serlin rsb by early 1990 several database system have achieve the 1000 tp milestone by the late 1990 s cluster of 100 machine be deliver over 10000 tpsa lsb scalability rsb long before then tpca be replace by the more challenging tpcc benchmark lsb tpcc rsb lsb levine rsb tpcc have a similar experience the early system deliver 1k tpmc at 2000 tpmc today system be deliver about to 3m tpmc for about 5 tpmc transaction per second tpcc late dollar teller	CoRR	
819862	jim_gray	a measure of transaction processing 20 year later	figure 1 lrb by charles levine from lsb 2 rsb rrb priceperformance trend line for tpca and tpcc the 15year trend line track moore s law lrb 100x per 10 year rrb	CoRR	
819863	jim_gray alexander_s._szalay gyorgy_fekete	use table value function in sql server 2005 to implement a spatial data library	this article explain how to add spatial search function lrb pointnearpoint and point in polygon rrb to microsoft sql server 2005 use c and tablevalued function it be possible to use this library to add spatial search to you application without write any special code the library implement the publicdomain c hierarchical triangular mesh lrb htm rrb algorithm from johns hopkins university that c library be connect to sql server 2005 via a set of scalarvalued and tablevalued function these function act as a spatial index the sample package include a 11 mb sample spatial database of united states city and riverflow gauge the sample query from the sql testscriptsql article a visual studio 2005 project htmsln with all the sql and c code a paper doc tablevaluedfunctionsdoc a article doc htmcsharpdoc that provide a manual page for each routine a article doc htmdoc that explain the hierarchical triangular mesh algorithm in detail a article doc theregoestheneighborhooddoc which explain how the htm algorithm be use in astronomy this article also explain two other approach zone for batchoriented pointtopoint and pointarea comparison and region for do boolean algebra on area public domain implementation of those approach implement for sql server be use in the skyserver a popular astronomy website for the sloan digital sky survey lrb httpskyserversdssorg and by several other astronomy datum server sql server polygon skyserver astronomer spatial index	CoRR	
819864	alexander_s._szalay jim_gray george_fekete peter_z._kunszt peter_kukol ani_thakar	indexing the sphere with the hierarchical triangular mesh	we describe a method to subdivide the surface of a sphere into spherical triangle of similar but not identical shape and size the hierarchical triangular mesh lrb htm rrb be a quadtree that be particularly good at support search at different resolution from arc seconds to hemisphere the subdivision scheme be universal provide the basis for address and for fast lookup the htm provide the basis for a efficient geospatial indexing scheme in relational database where the datum have a inherent location on either the celestial sphere or the earth the htm index be superior to cartographical method use coordinate with singularity at the pole we also describe a way to specify surface region that efficiently represent spherical query area this article present the algorithm use to identify the htm triangle cover such region spatial indexing geographic sphere htm triangle	CoRR	
819865	gordon_bell jim_gray alexander_s._szalay	petascale computational systems	a balanced cyberinfrastructure be necessary to meet grow dataintensitive scientific need we believe that available resource should be allocate to benefit the broadest crosssection of the scientific community give the powerlaw distribution of problem size this mean that about half of funding agency resource should be spend on tier1 center at the petascale level and the other half dedicate to tier2 and tier3 center on a costsharing basis funding agency should support balanced system not just cpu farm as well as petascale io and networking they should also allocate resource for a balanced tier1 through tier3 cyberinfrastructure doi 101109 mc 200629 balanced flood byte empirical theoretical	CoRR	Microsoft_Research
819866	jim_gray catharine_van_ingen	empirical measurement of disk failure rate and error rates	the sata advertise bit error rate of one error in 10 terabyte be frightening we move 2 pb through lowcost hardware and see five disk read error event several controller failure and many system reboot cause by security patch we conclude that sata uncorrectable read error be not yet a dominant systemfault source they happen but be rare compare to other problem we also conclude that uer lrb uncorrectable error rate rrb be not the relevant metric for we need when a uncorrectable read error happen there be typically several damage storage block lrb and many uncorrectable read error rrb also some uncorrectable read error may be mask by the operating system the more meaningful metric for datum architect be mean time to datum loss lrb mttdl rrb disk operating	CoRR	
819867	maria_a._nieto-santisteban aniruddha_r._thakar alexander_s._szalay jim_gray	largescale query and xmatch enter the parallel zone	current and future astronomical survey be produce catalog with million and billion of object online access to such big dataset for datum mining and crosscorrelation be usually as highly desire as unfeasible provide these capability be become critical for the virtual observatory framework in this paper we present various performance test that show how use relational database management systems lrb rdbms rrb and a zone algorithm to partition and parallelize the computation we can facilitate largescale query and crossmatch spatial locality crossmatch largescale nbody interoperable	CoRR	
819868	russell_sears catharine_van_ingen jim_gray	to blob or not to blob large object storage in a database or a filesystem	application designer must decide whether to store large object lrb blob rrb in a filesystem or in a database generally this decision be base on factor such as application simplicity or manageability often system performance affect these factor folklore tell we that database efficiently handle large number of small object while filesystem be more efficient for large object where be the breakeven point when be access a blob store as a file cheaper than access a blob store as a database record the simple answer be blob smaller than 256kb be more efficiently handle by a database while a filesystem be more efficient for those greater than 1mb of course this will vary between different database and filesystem by measure the performance of a storage server that mimic common workload we find that the breakeven point depend on many factor however we experiment suggest that storage age the ratio of byte in delete object to byte in live object be dominant as storage age increase fragmentation tend to increase the filesystem we study have better fragmentation control than the database we use suggest the database system would benefit from incorporate idea from filesystem design conversely filesystem performance may be improve by use database technique to handle many small file surprisingly for these study when average object size be hold constant the distribution of object size do not significantly affect performance we also find that in addition to low percentage free space a low ratio of free space to average object size lead to fragmentation and performance degradation filesystem large object blob fragmentation ntf disk	CoRR	
819870	katalin_szlavecz andreas_terzis stuart_ozer razvan_musaloiu-elefteri joshua_cogan sam_small randal_c._burns jim_gray alexander_s._szalay	life under you feet a endtoend soil ecology sensor network database web server and analysis service	1 wireless sensor network can revolutionize soil ecology by provide measurement at temporal and spatial granularity previously impossible this paper present a soil monitoring system we develop and deploy at a urban forest in baltimore as a first step towards realize this vision mote in this network measure and save soil moisture and temperature in situ every minute raw measurement be periodically retrieve by a sensor gateway and store in a central database where calibrate version be derive and store the measurement database be publish through web services interface in addition analysis tool let scientist analyze current and historical datum and help manage the sensor network the article describe the system design what we learn from the deployment and initial result obtain from the sensor the system measure soil factor with unprecedented temporal precision however the deployment require devicelevel programming sensor calibration across space and time and crossreferencing measurement with external source the database web server and data analysis design require considerable innovation and expertise so the ratio of computerscientist to ecologist be 31 before sensor network can fulfill they potential as instrument that can be easily deploy by scientist these technical problem must be address so that the ratio be one nerd per ten ecologist wsn environmental monitor ecologist gateway revolution	CoRR	
819871	jim_gray maria_a._nieto-santisteban alexander_s._szalay	the zone algorithm for finding pointsnearapoint or crossmatching spatial datasets	zone index a ndimensional euclidian or metric space to efficiently support pointsnearapoint query either within a dataset or between two dataset the approach use relational algebra and the btree mechanism find in almost all relational database system hence the zone algorithm give a portablerelational implementation of pointsnearpoint spatial crossmatch and selfmatch query this article correct some mistake in a earlier article we write on the zone algorithm and describe some algorithmic improvement the appendix include a implementation of pointnearpoint selfmatch and crossmatch use the usgs city and stream gauge database crossmatch spatial locality nbody selfmatch	CoRR	
819872	jim_gray alexander_s._szalay tamas_budavari robert_lupton maria_a._nieto-santisteban ani_thakar	crossmatching multiple spatial observation and deal with missing data	crossmatch spatially cluster and organize several astronomical pointsource measurement from one or more survey ideally each object would be find in each survey unfortunately the observation condition and the object themselves change continually even some stationary object be miss in some observation sometimes object have a variable light flux and sometimes the see be worse in most case we be face with a substantial number of difference in object detection between survey and between observation take at different time within the same survey or instrument deal with such miss observation be a difficult problem the first step be to classify miss as ephemeral when the object move or simply disappear mask when noise hide or corrupt the object observation or edge when the object be near the edge of the observational field this classification and a spatial library to represent and manipulate observational footprint help construct a match table record both hit and miss transitive closure cluster friendsoffriend into object bundle the bundle summary statistics be record in a bundle table this design be a evolution of the sloan digital sky survey crossmatch design that compare overlap observation take at different time give several observation of the sky call run astronomer often want to crossmatch all the observation of each object from all run that observe that object a typical first step be to process the run to make a object catalog the catalog entry typically take the form two object be say to match if they come from different run and if they position differ by less than they classification distance pick the classification distance depend on the datum and on the intend use of the crossmatch if only stationary object be to be match then the classification distance can be a small multiple of the maximum of the two object s circular rm position error the position uncertainty or astrometric precision be often a constant for all object of a observation but when compare datum from different instrument or from time with different see the position uncertainty may differ various systematic effect can add to uncertainty a rigorous statistical argument base on mean density and other parameter can recommend a optimal bayes classification distance give a point in one run the probability in find another point at a separation r in another run give perfect accuracy be the sum of a dirac delta for the object run different time stationary object crossmatch	CoRR	
819873	vik_singh jim_gray ani_thakar alexander_s._szalay jordan_raddick bill_boroski svetlana_lebedeva brian_yanny	skyserver traffic report the first five year	the skyserver be a internet portal to the sloan digital sky survey catalog archive server from 2001 to 2006 there be a million visitor in 3 million session generate 170 million web hit 16 million adhoc sql query and 65 million page view the site currently average 35 thousand visitor and 400 thousand session per month the web and sql log be public we analyze traffic and session by duration usage pattern datum product and client type lrb mortal or bot rrb over time the analysis show lrb 1 rrb the site s popularity lrb 2 rrb the educational website that deliver nearly fifty thousand hour of interactive instruction lrb 3 rrb the relative use of interactive programmatic and batchlocal access lrb 4 rrb the success of offer adhoc sql personal database and batch job access to scientist as part of the datum publication lrb 5 rrb the continue interest in old dataset lrb 6 rrb the usage of sql construct and lrb 7 rrb a novel approach of use the corpus of correct sql query to suggest similar but correct statement when a user present a incorrect sql statement sdss data edr visitor skyserver sql	CoRR	
822162	jim_gray david_t._liu maria_a._nieto-santisteban alexander_s._szalay david_j._dewitt gerd_heber	scientific data management in the coming decade	scientific instrument and computer simulation be create vast datum store that require new scientific method to analyze and organize the datum datum volume be approximately double each year since these new instrument have extraordinary precision the datum quality be also rapidly improve analyze this datum to find the subtle effect miss by previous study require algorithm that can simultaneously deal with huge dataset and that can find very subtle effect find both needle in the haystack and finding very small haystack that be undetected in previous measurement doi 101145 11074991107503 metadata large raw subtle effect exten scientific data analysis tool computational resource	CoRR	
822163	peter_kukol jim_gray	performance considerations for gigabyte per second transcontinental disktodisk file transfers	move datum from cern to pasadena at a gigabyte per second use the next generation internet require good networking and good disk io ten gbps ethernet and oc192 link be in place so now it be simply a matter of programming this report describe we preliminary work and measurement in configure the disk subsystem for this effort use 24 sata disk at each endpoint we be able to locally read and write a ntfs volume be striped across 24 disk at 12 gbp a 32disk stripe deliver 17 gbp experiment on higher performance and highercapacity system deliver up to 35 gbp summary we have be work with cal tech lrb yang xia harvey newman et al rrb and cern lrb sylvain ravot et al rrb to move datum between cern and pasadena at 1gbp use the internet rather than sneaker net we network colleague lrb ahmed talat inder sethi et al rrb have a good start on use 10 gbps ethernet to move 1gbp across the planet lrb ultralight rrb we lrb kukol and gray rrb be work on the firstmeter lastmeter problem of quickly move datum from disk to nic and nic to disk to do that we need roughly 12 gbp of disk io bandwidth lrb a 20 margin allow we some slack rrb that translate to about 20 disk drive at the outer band lrb 60gbpsdisk rrb and 34 drive when read the inner disk zone lrb 36 gbpsdisk rrb cern nic gbp networking disk	CoRR	
822164	tom_barclay jim_gray	terraserver sancluster architecture and operations experience	microsoft terraserver display aerial satellite and topographic image of the earth in a sql database available via the internet it be one of the most popular online atlase present seventeen terabyte of image datum from the united states geological survey lrb usgs rrb initially deployed in 1998 the system demonstrate the scalability of pc hardware and software windows and sql server on a single mainframeclass processor in september 2000 the backend database application be migrate to 4node activepassive cluster connect to a 18 terabyte storage area network lrb san rrb the new configuration be design to achieve 9999 availability for the backend application this paper describe the hardware and software component of the terraserver cluster and san and describe we experience in configure and operate this system for three year not surprisingly the hardware and architecture deliver better than four9 s of availability but operation mistake deliver three9 s sql server downtime hardware and software image datum terabyte	CoRR	
822165	jim_gray alexander_s._szalay	where the rubber meet the sky bridge the gap between database and science	scientist in all domain face a data avalanche both from better instrument and from improve simulation we believe that computer science tool and computer scientist be in a position to help all the science by building tool and develop technique to manage analyze and visualize petascale scientific information this article be summarize we experience over the last seven year try to bridge the gap between database technology and the need of the astronomy community in build the worldwide telescope avalanche astronomy archive scientific discipline generic	CoRR	
822166	peter_kukol jim_gray	sequential file programming patterns and performance with net	program pattern for sequential file access in the net framework be describe and the performance be measure the default behavior provide excellent performance on a single disk 50 mbp both reading and writing use large request size and do file preallocation when possible have quantifiable benefit when one consider disk array net unbuffered io deliver 800 mbp on a 16disk array but buffer io deliver about 12 of that performance consequently highperformance file and database utility be still force to use unbuffered io for maximum sequential performance the report be accompany by downloadable source code that demonstrate the concept and code that be use to obtain these measurement mbp net default behavior operating download	CoRR	
822172	maria_a._nieto-santisteban alexander_s._szalay aniruddha_r._thakar william_o'mullane jim_gray james_annis	when database systems meet the grid	we illustrate the benefit of combine database system and grid technology for dataintensive application use a cluster of sql server we reimplement a exist grid application that find galaxy cluster in a large astronomical database the sql implementation run a order of magnitude faster than the earlier tclcfilebased implementation we discuss why and how grid application can take advantage of database system metadata large amount of datum database systems avalanche sql implementation workflow dataintensive application	CoRR	
822226	william_o'mullane nolan_li maria_a._nieto-santisteban alexander_s._szalay ani_thakar jim_gray	batch be back casjob serve multitb datum on the web	the sloan digital sky survey lrb sdss rrb science database describe over 230 million object and be over 16 tb in size the sdss catalog archive server lrb cas rrb provide several level of query interface to the sdss datum via the skyserver website most query execute in seconds or minute however some query can take hour or day either because they require nonindex scan of the largest table or because they request very large result set or because they represent very complex aggregation of the datum these monster query not only take a long time they also affect response time for everyone else one or more of they can clog the entire system to ameliorate this problem we develop a multiserver multiqueue batch job submission execution and tracking system for the cas call casjobs the transfer of very large result set from query over the network be another serious problem statistics suggest that much of this datum transfer be unnecessary user would prefer to store result locally in order to allow further join and filter to allow local analysis a system be develop that give user they own personal database lrb mydb rrb at the server side user may transfer datum to they mydb and then perform further analysis before extract it to they own machine mydb table also provide a convenient way to share result of query with collaborator without download they casjobs be build use soap xml web service and have be in operation since may 2004 doi 101109 icws 200529 sdss datum	CoRR	Johns_Hopkins_Univ. USA
822474	jacek_becla andrew_hanushevsky sergei_nikolaev ghaleb_abdulla alexander_s._szalay maria_a._nieto-santisteban ani_thakar jim_gray	designing a multipetabyte database for lsst	the 32 gigapixel lsst camera will produce approximately half a petabyte of archive image every month these datum need to be reduce in under a minute to produce realtime transient alert and then add to the cumulative catalog for further analysis the catalog be expect to grow about three hundred terabyte per year the datum volume the realtime transient alert requirement of the lsst and its spatiotemporal aspect require innovative technique to build a efficient datum access system at reasonable cost as currently envision the system will rely on a database for catalog and metadata several database system be be evaluate to understand how they perform at these datum rate datum volume and access pattern this paper describe the lsst requirement the challenge they impose the datum access philosophy result to date from evaluate available database technology against lsst requirement and the propose database architecture to meet the datum challenge doi 101117 12671721 metadata petabyte lsst astronomy terabyte	CoRR	
832561	jim_gray paul_r._mcjones mike_w._blasgen bruce_g._lindsay raymond_a._lorie thomas_g._price gianfranco_r._putzolu irving_l._traiger	the recovery manager of the system r database manager	the recovery subsystem of a experimental datum management system be describe and evaluate the transactmn concept allow application program to commit abort or partially undo they effect the doundoredo protocol allow new recoverable type and operation to be add to the recovery system apphcation program can record datum m the transaction log to facilitate applicationspecific recovery transaction undo and redo be base on record keep in a transaction log the checkpoint mechanism be base on differential fries lrb shadow rrb the recovery log be record on disk rather than tape make computer easier to use be the goal of most software database management system in particular provide a programming interface to ease the task of write electronic bookkeeping program the recovery manager of such a system in turn ease the task of write faulttolerant application program system r lsb astr76 rsb be a database system which support the relational model of datum the sql language lsb cham76 rsb provide operator that manipulate the database typically a user write a pli or cobol program which have imbed sql statement a collection of such statement be require to make a consistent transformation of the database to transfer fund from one account to another for example require two sql statement one to debit the first account and one to credit the second account in addition the transaction probably record the transfer in a history file for later reporting and for auditing purpose figure 1 give a example of such a program write in pseudopli the program effect a consistent transformation of the book of a hypothetical bank its action be either to discover a error accept the input message and produce a failure message or to discover no error accept the input message permismon to copy without fee all or part of this material be grant provide that the copy be not make or cent hstnbute for direct commercial advantage the acm copyright notme and the title of the publication and its date appear and notme be give that copying be by pernusmon of the association for computing machinery to copy otherwise or to republish reqmre a fee andor specific permission a v debit the source account by amount credit the destination account by amount record the transaction in a history file and produce a success message the programmer who write such a program ensure its correctness by ensure that it doi 101145 356842356847 wal database systems shadow paging disk file systems	ACM_Comput._Surv.	
839619	jim_gray surajit_chaudhuri adam_bosworth andrew_layman don_reichart murali_venkatrao frank_pellow hamid_pirahesh	datum cube a relational aggregation operator generalize groupby crosstab and sub total	data analysis application typically aggregate datum across many dimension look for anomaly or unusual pattern the sql aggregate function and the group by operator produce zerodimensional or onedimensional aggregate application need the ndimensional generalization of these operator this paper define that operator call the datum cube or simply cube the cube operator generalize the histogram crosstabulation rollup drilldown and subtotal construct find in most report writer the novelty be that cube be relation consequently the cube operator can be imbed in more complex nonprocedural data analysis program the cube operator treat each of the n aggregation attribute as a dimension of nspace the aggregate of a particular set of attribute value be a point in this space the set of point form a ndimensional cube superaggregate be compute by aggregate the ncube to lower dimensional space this paper lrb 1 rrb explain the cube and rollup operator lrb 2 rrb show how they fit in sql lrb 3 rrb explain how user can define new aggregate function for cube and lrb 4 rrb discuss efficient technique to compute the cube many of these feature be be add to the sql standard doi 101023 a 1009726021843 datum analysis groupby cube anomaly sql	Data_Min._Knowl._Discov.	Microsoft_Corp. Redmond WA
844438	timothy_c._k._chou jim_gray	transaction acceleration		IEEE_Database_Eng._Bull.	
844613	jim_gray	a measure of transaction processing 20 year later	figure 1 lrb by charles levine from lsb 2 rsb rrb priceperformance trend line for tpca and tpcc the 15year trend line track moore s law lrb 100x per 10 year rrb	IEEE_Data_Eng._Bull.	
844615	jim_gray alexander_s._szalay	where the rubber meet the sky bridge the gap between database and science	scientist in all domain face a data avalanche both from better instrument and from improve simulation we believe that computer science tool and computer scientist be in a position to help all the science by building tool and develop technique to manage analyze and visualize petascale scientific information this article be summarize we experience over the last seven year try to bridge the gap between database technology and the need of the astronomy community in build the worldwide telescope avalanche astronomy archive scientific discipline generic	IEEE_Data_Eng._Bull.	
845478	jim_gray	what next a few remain problem in information technlogy sigmod conference 1999 acm turing award lecture video		ACM_SIGMOD_Digital_Symposium_Collection	
856314	jim_gray	review benchmarking database systems a systematic approach		ACM_SIGMOD_Digital_Review	
888067	jim_gray michael_a._harrison	the theory of sequential relations		Information_and_Control	
888068	jim_gray michael_a._harrison oscar_h._ibarra	twoway pushdown automata		Information_and_Control	
891747	mike_w._blasgen morton_m._astrahan donald_d._chamberlin jim_gray w._frank_king_iii bruce_g._lindsay raymond_a._lorie james_w._mehl thomas_g._price gianfranco_r._putzolu mario_schkolnick patricia_g._selinger donald_r._slutz h._raymond_strong irving_l._traiger bradford_w._wade robert_a._yost	system r a architectural overview		IBM_Systems_Journal	
891750	mike_w._blasgen morton_m._astrahan donald_d._chamberlin jim_gray w._frank_king_iii bruce_g._lindsay raymond_a._lorie james_w._mehl thomas_g._price gianfranco_r._putzolu mario_schkolnick patricia_g._selinger donald_r._slutz h._raymond_strong irving_l._traiger bradford_w._wade robert_a._yost	system r a architectural overview		IBM_Systems_Journal	IBM_at_10215_Fernwood_Road Bethesda MD_20034 USA
929341	tom_barclay jim_gray steve_ekblad eric_strand jeffrey_richter	designing and building terraservice	a few simple rule guide the design of web service such as terraservice a geospatial service add to microsoft s popular terraserver database by stick to standardsbased tool the author be able to implement the web service with no major structural change to the database because it conform to the 1to10 kbyte in one second guideline terraservice offer a highly acceptable user experience the success of two us department of agriculture application that bridge the web service and the database demonstrate the astuteness of the author design principle doi 101109 mic 200695	IEEE_Internet_Computing	Microsoft
950518	jim_gray	what next a dozen informationtechnology research goal	charles babbage s vision of computing have largely be realize we be on the verge of realize vannevar bush s memex but we be some distance from pass the ture test these three vision and they associate problem have provide longrange research goal for many of we for example the scalability problem have motivate i for several decade this talk define a set of fundamental research problem that broaden the babbage bush and turing vision they extend babbage s computational goal to include highlysecure highlyavailable selfprogramming selfmanaging and selfreplicating system they extend bush s memex vision to include a system that automatically organize index digest evaluate and summarize information lrb as well as a human might rrb another group of problem extend turing s vision of intelligent machine to include prosthetic vision speech hearing and other sens each problem be simply state and each be orthogonal from the other though they share some common core technology doi 101145 602382602401 human expert	J._ACM	Microsoft_Research San_Francisco California
950520	jim_gray michael_a._harrison	on the covering and reduction problem for contextfree grammars	a formal definition of one grammar cover another grammar be present it be argue that this definition have the property that g cover g when and only when the ability to parse g suffice for parse g it be show that every grammar may be cover by a grammar in canonical two form every afree grammar be cover by a operator normal form grammar while there exist grammar which can not be cover by any grammar in greibach form any grammar may be cover by a invertible grammar each afree and chain reduce lr lrb k rrb lrb bound right context rrb grammar be cover by a precedence detectable lr lrb k rrb lrb bound right context rrb reducible grammar doi 101145 321724321732	J._ACM	
950521	jim_gray michael_a._harrison	canonical precedence scheme	a general theory of canonical precedence analysis be define and study the familiar type of precedence analysis such as operator precedence or simple precedence occur as special case of this theory among the theoretical result obtain be a characterization of the structure of precedence relation and the relation of	J._ACM	
971987	herve_gallaire jim_gray michael_a._harrison gabor_t._herman	infinite linear sequential machine		J._Comput._Syst._Sci.	
1048746	jim_gray	a conversation with tim bray	tim bray s waterloo be no crush defeat but rather the beginning of he success as one of the conqueror of search engine technology and xml in 1986 after work in software at dec and gte he take a job at the university of waterloo in ontario canada where he manage the new oxford english dictionary project a ambitious research endeavor to bring the venerable oxford english dictionary into the computer age doi 101145 10469311046941	ACM_Queue	
1048748	jim_gray mark_compton	a call to arm		ACM_Queue	
1064531	tom_barclay robert_barnes jim_gray prakash_sundaresan	load database use dataflow parallelism	this paper describe a parallel database load prototype for digital s rdb database product the prototype take a dataflow approach to database parallelism it include a i explorer i that discover and record the cluster configuration in a database a i client i cui interface that gather the load job description from the user and from the rdb catalog and a i optimizer i that pick the best parallel execution plan and record it in a i web i datum structure the web describe the datum i operator i the i dataflow river i among they the binding of operator to process process to processor and file to disc and tape this paper describe the optimizer s costbased hierarchical optimization strategy in some detail the prototype execute the web s plan by spawn a i web manager i process at each node of the cluster the manager create the local i executor i process and orchestrate startup phase checkpoint and shutdown the execution process perform one or more operator datum flow among the operator be via i memorytomemory stream i within a node and via webmanager multiplexed tcpip stream among node the design of the i transaction i and i checkpointrestart i mechanism be also describe preliminary measurement indicate that this design will give excellent scaleup doi 101145 190627190647 optimizer checkpoint	SIGMOD_Record	
1064679	philip_a._bernstein michael_l._brodie stefano_ceri david_j._dewitt michael_j._franklin hector_garcia-molina jim_gray gerald_held joseph_m._hellerstein h._v._jagadish michael_lesk david_maier jeffrey_f._naughton hamid_pirahesh michael_stonebraker jeffrey_d._ullman	the asilomar report on database research	the database research community be rightly proud of success in basic research and its remarkable record of technology transfer now the field need to radically broaden its research focus to attack the issue of capture store analyze and present the vast array of online datum the database research community should embrace a broader research agenda 8212 broaden the definition of database management to embrace all the content of the web and other online datum store and rethink we fundamental assumption in light of technology shift to accelerate this transition we recommend change the way research result be evaluate and present in particular we advocate encourage more speculative and longrange work move conference to a poster format and publish all research literature on the web doi 101145 306101306137 database research xml asilomar report database community main memory	SIGMOD_Record	
1064680	philip_a._bernstein umeshwar_dayal david_j._dewitt dieter_gawlick jim_gray matthias_jarke bruce_g._lindsay peter_c._lockemann david_maier erich_j._neuhold andreas_reuter lawrence_a._rowe hans-jorg_schek joachim_w._schmidt michael_schrefl michael_stonebraker	future direction in dbms research the laguna beach participant	on february 45 1988 the international computer science institute sponsor a two day workshop at which 16 senior member of the datum base research community discuss future research topic in the dbms area this paper summarize the discussion which take place doi 101145 3822721367994 database community data management systems graphical query interfaces	SIGMOD_Record	
1064875	david_j._dewitt jim_gray	parallel database systems the future of database processing or a pass fad	the concept of parallel database machine consist of exotic hardware have be replace by a fairly conventional sharednothing hardware base along with a highly parallel dataflow software architecture such a design provide speedup and scaleup in processing relational database query this paper review the technique use by such system and survey current commercial and research system doi 101145 122058122071 datum processing database machine thorough review rightdeep trees	SIGMOD_Record	
1065061	jim_gray goetz_graefe	the fiveminute rule ten year later and other computer storage rule of thumb	simple economic and performance argument suggest appropriate lifetime for main memory page and suggest optimal page size the fundamental tradeoff be the price and bandwidth of ram and disk the analysis indicate that with today s technology five minute be a good lifetime for randomly access page one minute be a good lifetime for twopass sequentially access page and 16 kb be a good size for index page these rulesofthumb change in predictable way as technology ratio change they also motivate the importance of the new italic kaps maps scan italic and italic kaps map tbscan italic metric doi 101145 271074271094 ram physical characteristic main memory extra block size memory hierarchy rent disk chart	SIGMOD_Record	
1065063	jim_gray david_t._liu maria_a._nieto-santisteban alexander_s._szalay david_j._dewitt gerd_heber	scientific datum management in the come decade	scientific instrument and computer simulation be create vast datum store that require new scientific method to analyze and organize the datum datum volume be approximately double each year since these new instrument have extraordinary precision the datum quality be also rapidly improve analyze this datum to find the subtle effect miss by previous study require algorithm that can simultaneously deal with huge dataset and that can find very subtle effect find both needle in the haystack and finding very small haystack that be undetected in previous measurement doi 101145 11074991107503 metadata large raw subtle effect exten scientific data analysis tool computational resource	SIGMOD_Record	Microsoft
1065693	michael_stonebraker lawrence_a._rowe bruce_g._lindsay jim_gray michael_j._carey michael_l._brodie philip_a._bernstein david_beech	thirdgeneration database system manifesto the committee for advanced dbms function	we call the older hierarchical and network system first generation datftha system and refer to the current collection of relmional system as the second generation in this paper we consider the characteristic that must be rus fie by the next generation of datum managex which we call third generation database system we requirement be collect into three ba iq tenet along with 13 more detailed proposilion doi 101145 101077390001 database systems sql objectrelational model	SIGMOD_Record	
1066145	mike_w._blasgen jim_gray michael_f._mitoma thomas_g._price	the convoy phenomenon	convoys describe when drive on a twolane road with no pass one often encounter cluster of car this be because a fastmoving car will soon bump into a slow one the equilibrium state of such a system be for everyone to be in a convoy behind the slowest car in system r lsb astrahan rsb transaction often bump into one another when contend for share resource this contention appear as conflict request for lock lrb you may know lock by the name semaphore monitor latch or queue rrb typically access to these resource follow the protocol lock resource operateonresource unlock resource if other process request the lock while it be grant then they be place in a queue of waiter and suspend when the lock become available request be grant in firstcome firstserved order set and clear a lock cost ten instuction if no waiting be involve if wait be involve it cost 50 instruction plus two process dispatch lrb ie several thousand instruction rrb lrb note the system r lock manager be much fancier than this lsb gray rsb lsb nauman rsb rrb in what follow three statistics about a particular lock will be of interest the execution interval of a lock be the average number of instruction execute between successive request for that lock by a process the duration of a lock be the average number of instruction execute while the lock be hold the collision cross section of a lock be the fraction of time it be grant in a uniprocessor the collision cross section be lrb duration lrb duration interval rrb rrb ignore the wait time and task switching time if a request must wait there be three hightraffic lock in system r regulate access to the buffer pool recovery log and to system entryexit estimate of these statistics for the high traffic resource of system r be show below doi 101145 850657850659 buffer lock convoy contention queue	Operating_Systems_Review	
1066315	jim_gray	increase the effectiveness of os research		Operating_Systems_Review	
1118209	morton_m._astrahan mike_w._blasgen donald_d._chamberlin kapali_p._eswaran jim_gray patricia_p._griffiths w._frank_king_iii raymond_a._lorie paul_r._mcjones james_w._mehl gianfranco_r._putzolu irving_l._traiger bradford_w._wade vera_watson	system r relational approach to database management	system r be a database management system which provide a high level relational datum interface the system provide a high level of datum independence by isolate the end user as much as possible from underlie storage structure the system permit definition of a variety of relational view on common underlie datum datum control feature be provide include authorization integrity assertion trigger transaction a log and recovery subsystem and facility for maintain datum consistency in a sharedupdate environment this paper contain a description of the overall architecture and design of the system at the present time the system be be implement and the design evaluate we emphasize that system r be a vehicle for research in database architecture and be not plan as a product doi 101145 320455320457 dbmss expr rss spec dbm	ACM_Trans._Database_Syst.	
1118425	jim_gray leslie_lamport	consensus on transaction commit	the distribute transaction commit problem require reach agreement on whether a transaction be commit or abort the classic twophase commit protocol block if the coordinator fail faulttolerant consensus algorithm also reach agreement but do not block whenever any majority of the process be work the paxos commit algorithm run a paxos consensus algorithm on the commitabort decision of each participant to obtain a transaction commit protocol that use 2 i f i plus 1 coordinator and make progress if at least i f i plus 1 of they be work properly paxos commit have the same stablestorage write delay and can be implement to have the same message delay in the faultfree case as twophase commit but it use more message the classic twophase commit algorithm be obtain as the special i f i equal 0 case of the paxos commit algorithm doi 101145 11328631132867 2pc locking leader paxos coordinator	ACM_Trans._Database_Syst.	Microsoft_Research San_Francisco CA
1118715	irving_l._traiger jim_gray cesare_a._galtieri bruce_g._lindsay	transaction and consistency in distribute database systems	the concept of transaction and of datum consistency be define for a distribute system the case of partition datum where fragment of a file be store at multiple node and replicate datum where a file be replicate at several node be discuss it be argue that the distribution and replication of datum should be transparent to the program which use the datum that be the programming interface should provide location transparency replica transparency concurrency transparency and failure transparency technique for provide such transparency be abstracted and discuss by extend the notion of system schedule and system clock to handle multiple node it be show that a distribute system can be model as a single sequential execution sequence this model be then use to discuss simple technique for implement the various form of transparency doi 101145 319732319734 concurrency database systems transparency several node atomicity	ACM_Trans._Database_Syst.	
1128267	jim_gray	a approach to decentralized computer systems	the technology for distribute computing be available however it be argue that decentralize system will always require more careful design planning and management than they centralized counterpart the rationale for and against decentralization be give and a technical approach to decentralize system be sketch this approach contrast with the popular concept of a distribute integrate database which transparently provide remote io against single system image rather it propose that function be distribute as server which abstract datum as highlevel operation on object and communicate with requestor via a standard message protocol this requestorserver approach have the advantage of modularity and performance doi 101109 tse 19866312966	IEEE_Trans._Software_Eng.	
1138239	chris_nyberg tom_barclay zarka_cvetanovic jim_gray david_b._lomet	alphasort a cachesensitive parallel external sort	a new sort algorithm call alphasort demonstrate that commodity processor and disk can handle commercial batch workload use commodity processor memory and array of scsi disk alphasort run the industrystandard sort benchmark in seven seconds this beat the best publish record on a 32cpu 32disk hypercube by 81 on another benchmark alphasort sort more than a gigabyte in one minute alphasort be a cachesensitive memoryintensive sort algorithm we argue that modern architecture require algorithm designer to reexamine they use of the memory hierarchy alphasort use cluster datum structure to get good cache locality file striping to get high disk bandwidth quicksort to generate run and replacementselection to merge the run it use shared memory multiprocessor to break the sort into subsort chore because startup time be become a significant part of the total time we propose two new benchmark lrb 1 rrb minutesort how much can you sort in one minute and lrb 2 rrb pennysort how much can you sort for one penny run speedup terabyte disk striping	VLDB_J.	
1179249	magdalena_balazinska amol_deshpande michael_j._franklin phillip_b._gibbons jim_gray mark_h._hansen michael_liebhold suman_nath alexander_s._szalay vincent_tao	data management in the worldwide sensor web	harvest the benefit of a sensorrich world present many datum management challenge recent advance in research and industry aim to address these challenge with the rapidly increase number of largescale sensor network deployment the vision of a worldwide sensor web be close to become a reality doi 101109 mprv 200727 wsn outbreak sensor data networking data quality	IEEE_Pervasive_Computing	Washington_Univ. St._Louis MO
1196557	jim_gray vera_watson	a share segment and interprocess communication facility for vm370		IBM_Research_Report	
1196564	irving_l._traiger jim_gray cesare_a._galtieri bruce_g._lindsay	transaction and consistency in distribute database systems	the concept of transaction and of datum consistency be define for a distribute system the case of partition datum where fragment of a file be store at multiple node and replicate datum where a file be replicate at several node be discuss it be argue that the distribution and replication of datum should be transparent to the program which use the datum that be the programming interface should provide location transparency replica transparency concurrency transparency and failure transparency technique for provide such transparency be abstracted and discuss by extend the notion of system schedule and system clock to handle multiple node it be show that a distribute system can be model as a single sequential execution sequence this model be then use to discuss simple technique for implement the various form of transparency doi 101145 319732319734 concurrency database systems transparency several node atomicity	IBM_Research_Report	
1203997	jim_gray	the benchmark handbook for database and transaction systems lrb 1st edition rrb		null	
1203998	jim_gray	the benchmark handbook for database and transaction systems lrb 2nd edition rrb	cloud computing promise a number of advantage for the deployment of dataintensive application one important promise be reduce cost with a payasyougo business model another promise be lrb virtually rrb unlimited throughput by add server if the workload increase this paper list alternative architecture to effect cloud computing for database application and report on the result of a comprehensive evaluation of exist commercial cloud service that have adopt these architecture the focus of this work be on transaction processing lrb ie read and update workload rrb rather than analytic or olap workload which have recently gain a great deal of attention the result be surprising in several way most importantly it seem that all major vendor have adopt a different architecture for they cloud service as a result the cost and performance of the service vary significantly depend on the workload doi 101145 18071671807231 cloud service rds vendor transaction processing ing	null	
1203999	jim_gray andreas_reuter	transaction processing concept and technique		null	
1227155	jim_gray bob_fitzgerald	flash disk opportunity for server application	future flashbased disk could provide breakthrough in iop power reliability and volumetric capacity when compare with conventional disk nand flash density have be double each year since 1996 samsung announce that its 32gigabit nand flash chip would be available in 2007 this be consistent with changgyu hwang s flash memory growth model1 that nand flash density will double each year until 2010 hwang recently extend that 2003 prediction to 2012 suggest 64 time the current density250 gb per chip this be hard to credit but hwang and samsung have deliver 16 time since he 2003 article when 2gb chip be just emerge so we should be prepare for the day when a flash drive be a terabyte lrb rrb as hwang point out in he article mobile and consumer application rather than the pc ecosystem be push this technology doi 101145 14132541413261 random nand ssd flash disk	ACM_Queue	
1227187	jim_gray	distribute computing economics	computing economics be change today there be rough price parity between lrb 1 rrb one database access lrb 2 rrb 10 byte of network traffic lrb 3 rrb 100000 instruction lrb 4 rrb 10 byte of disk storage and lrb 5 rrb a megabyte of disk bandwidth this have implication for how one structure internetscale distribute computing one put computing as close to the datum as possible in order to avoid expensive network traffic doi 101145 13941271394131 ist tcp computing resource administrator byte	ACM_Queue	Microsoft_Research
1235303	jim_gray	the benchmark handbook for database and transaction systems lrb 1st edition rrb		null	
1235304	jim_gray	the benchmark handbook for database and transaction systems lrb 2nd edition rrb	cloud computing promise a number of advantage for the deployment of dataintensive application one important promise be reduce cost with a payasyougo business model another promise be lrb virtually rrb unlimited throughput by add server if the workload increase this paper list alternative architecture to effect cloud computing for database application and report on the result of a comprehensive evaluation of exist commercial cloud service that have adopt these architecture the focus of this work be on transaction processing lrb ie read and update workload rrb rather than analytic or olap workload which have recently gain a great deal of attention the result be surprising in several way most importantly it seem that all major vendor have adopt a different architecture for they cloud service as a result the cost and performance of the service vary significantly depend on the workload doi 101145 18071671807231 cloud service rds vendor transaction processing ing	null	
1235305	jim_gray andreas_reuter	transaction processing concept and technique		null	
1269959	alexander_s._szalay ani_r._thakar jim_gray	the sqlloader dataloading pipeline	use a database management system lrb dbms rrb be essential to ensure the datum integrity and reliability of large multidimensional datum set however load multiterabyte datum into a dbms be a timeconsuming and errorprone task that the author have try to automate by develop the sqlloader pipelinea distribute workflow system for datum load doi 101109 mcse 200818 csv escience copyright holder big data analytics application	Computing_in_Science_and_Engineering	Microsoft_Research
1269994	ani_r._thakar alexander_s._szalay george_fekete jim_gray	the catalog archive server database management system	the multiterabyte sloan digital sky survey s lrb sdss s rrb catalog datum be store in a commercial relational database management system with sql query access and a builtin query optimizer the sdss catalog archive server add advanced datum mining feature to the dbms to provide fast online access to the datum doi 101109 mcse 200815 copyright holder	Computing_in_Science_and_Engineering	Microsoft_Research
1412437	serge_abiteboul rakesh_agrawal phil_bernstein mike_carey stefano_ceri bruce_croft david_dewitt mike_franklin hector_garcia_molina dieter_gawlick jim_gray laura_haas alon_halevy joe_hellerstein yannis_ioannidis martin_kersten michael_pazzani mike_lesk david_maier jeff_naughton hans_schek timos_sellis avi_silberschatz mike_stonebraker rick_snodgrass jeff_ullman gerhard_weikum jennifer_widom stan_zdonik	the lowell database research selfassessment	database need be change drive by the internet and increase amount of scientific and sensor datum in this article the author propose research into several important new direction for database management system doi 101145 10607101060718 database research fuzzy logic globalization eg sql	Communications_of_the_ACM	
1441129	jim_gray	the transaction concept virtue and limitation	a transaction be a transformation of state which have the property of atomicity lrb all or nothing rrb durability lrb effect survive failure rrb and consistency lrb a correct transformation rrb the transaction concept be key to the structuring of datum management application the concept may have applicability to programming system in general this paper restate the transaction concept and attempt to put several implementation approach in perspective it then describe some area which require further study lrb 1 rrb the integration of the transaction concept with the notion of abstract datum type lrb 2 rrb some technique to allow transaction to be compose of subtransaction and lrb 3 rrb handle transaction which last for extremely long time lrb day or month rrb contract durability acid blockin atomicity	Readings_in_database_systems	Tandem_Computers_Inc. Cupertino CA
1469234	jim_gray franco_putzolu	the 5 minute rule for trading memory for disc access and the 10 byte rule for trading memory for cpu time	if a item be access frequently enough it should be main memory resident for current technology 8220 frequently enough 8221 mean about every five minute along a similar vein one can frequently trade memory space for cpu time for example bit can be pack in a byte at the expense of extra instruction to extract the bit it make economic sense to spend ten byte of main memory to save one instruction per second these result depend on current price ratio of processor memory and disc access these ratio be change and hence the constant in the rule be change doi 101145 3871338755 io memory resident reminiscent byte main memory disk	ACM_SIGMOD_Record	Tandem_Computers Cupertino CA
1534524	jim_gray andreas_reuter	transaction processing concept and technique 1st edition		null	
1544088	phil_bernstein michael_brodie stefano_ceri david_dewitt mike_franklin hector_garcia-molina jim_gray jerry_held joe_hellerstein h._v._jagadish michael_lesk dave_maier jeff_naughton hamid_pirahesh mike_stonebraker jeff_ullman	the asilomar report on database research	the database research community be rightly proud of success in basic research and its remarkable record of technology transfer now the field need to radically broaden its research focus to attack the issue of capture store analyze and present the vast array of online datum the database research community should embrace a broader research agenda 8212 broaden the definition of database management to embrace all the content of the web and other online datum store and rethink we fundamental assumption in light of technology shift to accelerate this transition we recommend change the way research result be evaluate and present in particular we advocate encourage more speculative and longrange work move conference to a poster format and publish all research literature on the web doi 101145 306101306137 database research xml asilomar report database community main memory	ACM_SIGMOD_Record	
1545278	chris_nyberg tom_barclay zarka_cvetanovic jim_gray dave_lomet	alphasort a cachesensitive parallel external sort	a new sort algorithm call alphasort demonstrate that commodity processor and disk can handle commercial batch workload use commodity processor memory and array of scsi disk alphasort run the industrystandard sort benchmark in seven seconds this beat the best publish record on a 32cpu 32disk hypercube by 81 on another benchmark alphasort sort more than a gigabyte in one minute alphasort be a cachesensitive memoryintensive sort algorithm we argue that modern architecture require algorithm designer to reexamine they use of the memory hierarchy alphasort use cluster datum structure to get good cache locality file striping to get high disk bandwidth quicksort to generate run and replacementselection to merge the run it use shared memory multiprocessor to break the sort into subsort chore because startup time be become a significant part of the total time we propose two new benchmark lrb 1 rrb minutesort how much can you sort in one minute and lrb 2 rrb pennysort how much can you sort for one penny run speedup terabyte disk striping	Readings_in_database_systems_(3rd_ed.)	
1545539	david_dewitt jim_gray	parallel database system the future of high performance database system	the success of these system refute a 1983 paper predict the demise of database machine lsb 3 rsb ten year ago the future of highly parallel database machine seem gloomy even to they staunchest advocate most database machine research have focus on specialize often trendy hardware such as ccd memory bubble memory headpertrack disk and optical disk none of these technology fulfil they promise so there be a sense that conventional cpus electronic ram and mcvinghead magnetic disk would dominate the scene for many year to come at that time disk throughput be predict to double while processor speed be predict to increase by much larger factor consequently critic predict that multiprocessor system would scxm be io limit unless a solution to the io bottleneck be find whiie these prediction be fairly accurate about the future of hardware the critic be certainly wrong about the overall future of parallel database system over the last decade eradata tandem and a host of startup company have successfully develop and market highly parallel machine doi 101145 129888129894 database systems sharednothing dbm disk ture	Readings_in_database_systems_(3rd_ed.)	
1548510	jim_gray surajit_chaudhuri adam_bosworth andrew_layman don_reichart murali_venkatrao frank_pellow hamid_pirahesh	datum cube a relational aggregation operator generalize groupby crosstab and subtotal	data analysis application typically aggregate datum across many dimension look for anomaly or unusual pattern the sql aggregate function and the group by operator produce zerodimensional or onedimensional aggregate application need the ndimensional generalization of these operator this paper define that operator call the datum cube or simply cube the cube operator generalize the histogram crosstabulation rollup drilldown and subtotal construct find in most report writer the novelty be that cube be relation consequently the cube operator can be imbed in more complex nonprocedural data analysis program the cube operator treat each of the n aggregation attribute as a dimension of nspace the aggregate of a particular set of attribute value be a point in this space the set of point form a ndimensional cube superaggregate be compute by aggregate the ncube to lower dimensional space this paper lrb 1 rrb explain the cube and rollup operator lrb 2 rrb show how they fit in sql lrb 3 rrb explain how user can define new aggregate function for cube and lrb 4 rrb discuss efficient technique to compute the cube many of these feature be be add to the sql standard doi 101023 a 1009726021843 datum analysis groupby cube anomaly sql	Readings_in_database_systems_(3rd_ed.)	Microsoft_Corp. Redmond WA
1551876	tom_barclay jim_gray don_slutz	microsoft terraserver a spatial datum warehouse	the terraserver store aerial satellite and topographic image of the earth in a sql database available via the internet it be the world s largest online atla combine five terabyte of image datum from the united states geological survey lrb usgs rrb and spin2 this report describe the systemredesign base on we experience over the last year it also report usage and operation result over the last year over 2 billion web hit and over 20 terabyte of imagry serve over the internet internet browser provide intuitive spatial and text interface to the datum user need no special hardware software or knowledge to locate and browse imagery this paper describe how terabyte of internet unfriendly geospatial image be scrub and edit into hundred of million of internet friendly image tile and load into a sql datum warehouse microsoft terraserver demonstrate that generalpurpose relational database technology can manage large scale image repository and show that web browser can be a good geospatial image presentation system satellite image datum sql database terabyte imagery	ACM_SIGMOD_Record	Microsoft_Research 301_Howard_St. Suite_830 San_Francisco CA
1553770	jim_gray pat_helland patrick_o'neil dennis_shasha	the danger of replication and a solution	update anywhereanytimeanyway transactional replication have unstable behavior as the workload scale up a tenfold increase in node and traffic give a thousand fold increase in deadlock or reconciliation master copy replication lrb primary copy rrb scheme reduce this problem a simple analytic model demonstrate these result a new twotier replication algorithm be propose that allow mobile lrb disconnect rrb application to propose tentative update transaction that be later apply to a master copy commutative update transaction avoid the instability of other replication scheme doi 101145 233269233330 synchronous replication deadlock lock replica sync	Readings_in_database_systems_(3rd_ed.)	
1574840	michael_stonebraker lawrence_a._rowe bruce_g._lindsay jim_gray michael_j._carey michael_l._brodie philip_a._bernstein david_beech	thirdgeneration database system manifesto	we call the older hierarchical and network system first generation datftha system and refer to the current collection of relmional system as the second generation in this paper we consider the characteristic that must be rus fie by the next generation of datum managex which we call third generation database system we requirement be collect into three ba iq tenet along with 13 more detailed proposilion doi 101145 101077390001 database systems sql objectrelational model	ACM_SIGMOD_Record	
1574963	jim_gray paul_mcjones mike_blasgen bruce_lindsay raymond_lorie tom_price franco_putzolu irving_traiger	the recovery manager of the system r database manager	the recovery subsystem of a experimental datum management system be describe and evaluate the transactmn concept allow application program to commit abort or partially undo they effect the doundoredo protocol allow new recoverable type and operation to be add to the recovery system apphcation program can record datum m the transaction log to facilitate applicationspecific recovery transaction undo and redo be base on record keep in a transaction log the checkpoint mechanism be base on differential fries lrb shadow rrb the recovery log be record on disk rather than tape make computer easier to use be the goal of most software database management system in particular provide a programming interface to ease the task of write electronic bookkeeping program the recovery manager of such a system in turn ease the task of write faulttolerant application program system r lsb astr76 rsb be a database system which support the relational model of datum the sql language lsb cham76 rsb provide operator that manipulate the database typically a user write a pli or cobol program which have imbed sql statement a collection of such statement be require to make a consistent transformation of the database to transfer fund from one account to another for example require two sql statement one to debit the first account and one to credit the second account in addition the transaction probably record the transfer in a history file for later reporting and for auditing purpose figure 1 give a example of such a program write in pseudopli the program effect a consistent transformation of the book of a hypothetical bank its action be either to discover a error accept the input message and produce a failure message or to discover no error accept the input message permismon to copy without fee all or part of this material be grant provide that the copy be not make or cent hstnbute for direct commercial advantage the acm copyright notme and the title of the publication and its date appear and notme be give that copying be by pernusmon of the association for computing machinery to copy otherwise or to republish reqmre a fee andor specific permission a v debit the source account by amount credit the destination account by amount record the transaction in a history file and produce a success message the programmer who write such a program ensure its correctness by ensure that it doi 101145 356842356847 wal database systems shadow paging disk file systems	ACM_Computing_Surveys_(CSUR)	Tandem_Computers 19333_Vallco_Parkway Cupertino California
1589364	banu_zden eran_gabber bruce_hillyer wee_teck_ng elizabeth_a._m._shriver david_j._dewitt bruce_gordon jim_gray john_wilkes	storage service providers a solution for storage management lrb panel rrb		Proceedings_of_the_27th_International_Conference_on_Very_Large_Data_Bases	
1607878	erik_riedel catharine_van_ingen jim_gray	a performance study of sequential io on window nttm4		Proceedings_of_the_2nd_conference_on_USENIX_Windows_NT_Symposium_-_Volume_2	Microsoft_Research
1630476	jim_gray bob_horst mark_walker	parity striping of disc array lowcost reliable storage with acceptable throughput	n analysis of mirror disc and of raid5 show that mirror have considerably better throughput measure as requestssecond on random request of arbitrary size lrb up to 1mb rrb mirror have comparable or better response time for request of reasonable size lrb less than lookb rrb but mirror have a 100 storage penalty store the datum twice parity striping be a data layout that stripe the parity across the disc but do not stripe the datum parity striping have throughput almost as good as mirror and have costlgb comparable to raids designscombing the advantage of both for hightraffic disc resident datum parity striping have additional fault containment and software benefit as well parity striping sacrifice the high datum transfer rate of raid design for high throughput it be argue that response time and throughput be preferable performance metric disc dni cop parity disk read	Proceedings_of_the_sixteenth_international_conference_on_Very_large_databases	
1666800	j._gray	a comparison of the byzantine agreement problem and the transaction commit problem	transaction commit and byzantine agreement solve the problem of multiple process reach agreement in the presence of process and message failure this paper summarize the computation and fault model of the two kind of agreement and show the difference between they in particular it explain that byzantine agreement be rarely use in practice because it involve significantly more hardware and message yet do not give predictable behavior if there be more than a few error	Fault-Tolerant_Distributed_Computing	
1709412	hal_berenson phil_bernstein jim_gray jim_melton elizabeth_o'neil	a critique of ansi sql isolation level	ansi sql92 lsb ms ansi rsb define isolation i level i in term of i phenomenon i dirty read nonrepeatable read and phantoms this paper show that these phenomenon and the ansi sql definition fail to properly characterize several popular isolation level include the standard lock implementation of the level cover ambiguity in the statement of the phenomenon be investigate and a more formal statement be arrive at in addition new phenomenon that better characterize isolation type be introduce finally a important multiversion isolation type call snapshot isolation be define doi 101145 223784223785 ansi serializability isolation level snapshot isolation ple	Proceedings_of_the_1995_ACM_SIGMOD_international_conference_on_Management_of_data	U.C._Berkeley
1735595	jim_gray	benchmark handbook for database and transaction processing systems	this book be a short complete summary of the most important approach to performance measurement of database system and transaction processing system it be intend to serve as a tutorial for the novice and a reference for the professional include be contribution by ten author dina bitton rick cattell david dewitt jim gray neal nelson patrick oneil tom sawyer omri serlin carolyn turbyfill and cyril orji doi 101145 1818401044953	null	
